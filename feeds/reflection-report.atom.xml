<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>FINA4350 Student Blog 2024 - Reflection Report</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/" rel="alternate"></link><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/feeds/reflection-report.atom.xml" rel="self"></link><id>https://buehlmaier.github.io/FINA4350-student-blog-2024-01/</id><updated>2024-04-30T22:30:00+08:00</updated><entry><title>Blog Post Two - Encounter, Analyze, and Solve (by Group "Textonomy")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/blog-post-two-encounter-analyze-and-solve-by-group-textonomy.html" rel="alternate"></link><published>2024-04-30T22:30:00+08:00</published><updated>2024-04-30T22:30:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-30:/FINA4350-student-blog-2024-01/blog-post-two-encounter-analyze-and-solve-by-group-textonomy.html</id><summary type="html">&lt;p&gt;As we are all new to NLP and related techniques, we believe sharing the challenges we met along the way is valuable. We hope these experiences can serve as a reference for future NLP learners.&lt;/p&gt;
&lt;h2&gt;Data Collection and Preprocessing (by Zepa)&lt;/h2&gt;
&lt;p&gt;In the data processing part for pre-training data, our …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we are all new to NLP and related techniques, we believe sharing the challenges we met along the way is valuable. We hope these experiences can serve as a reference for future NLP learners.&lt;/p&gt;
&lt;h2&gt;Data Collection and Preprocessing (by Zepa)&lt;/h2&gt;
&lt;p&gt;In the data processing part for pre-training data, our group has adopted data from 3 sources, namely &lt;strong&gt;Financial phrasebank&lt;/strong&gt; (financial news headings and sentiments), &lt;strong&gt;Sanders&lt;/strong&gt; (Twitter sentiment corpus), and &lt;strong&gt;Taborda&lt;/strong&gt; (stock market tweets data). &lt;/p&gt;
&lt;p&gt;One challenge here is that our group needs to select accurate, relevant, and useful data, and it will be best if those data are well-packed, cleaned, and tidied up already. Our group spent a certain amount of time selecting high-quality data input to ensure potentially high-quality output.&lt;/p&gt;
&lt;p&gt;Another little challenge here is that there may be an extra space at the last line of each individual processed sentence due to the usage of &lt;code&gt;\n&lt;/code&gt;, which is undesired, so our group modified the previous code of &lt;code&gt;code_taborada.py&lt;/code&gt; and utilized the &lt;code&gt;if-else&lt;/code&gt; statement to specifically make it excluded. Below is an example from &lt;code&gt;code_taborada.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="code snippet in code_taborada.py" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_code-snippet.png"&gt;&lt;/p&gt;
&lt;h2&gt;Model Training (By Bosco)&lt;/h2&gt;
&lt;p&gt;In this part, I will illustrate the challenges I face when training our BERT model with reference to the &lt;a href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert"&gt;"Classify text with BERT"&lt;/a&gt; tutorial on TensorFlow.&lt;/p&gt;
&lt;p&gt;The most challenging part is to read the dataset in python, which was collected and preprocessed by Zepa. Zepa combined the three datasets and transformed them into one consistent format, which consists of some lists of &lt;code&gt;[sentence, label]&lt;/code&gt;. The dataset is then exported as a &lt;code&gt;.txt&lt;/code&gt; file for later use. My job is to read the text file in Python and use the data to train the BERT model. Below is the code I used to read the lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;combined_result.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Remove leading and trailing whitespace and newline character&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c1"&gt;# Remove the square brackets&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Find the index of the last comma in the line&lt;/span&gt;
        &lt;span class="n"&gt;last_comma_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rindex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;last_comma_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;last_comma_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code assumes that each line in the text file is in the form of &lt;code&gt;[sentence, label]&lt;/code&gt;. It then proceeds to find the position of the last comma. Everything before the last comma is assigned to the sentence variable, and the integer after the last comma represents the label. One can easily extract the  i&lt;sup&gt;th&lt;/sup&gt; sentence by &lt;code&gt;data[i][0]&lt;/code&gt; and the  i&lt;sup&gt;th&lt;/sup&gt; label by &lt;code&gt;data[i][1]&lt;/code&gt;. The code functions effectively when the dataset is in a consistent format. I test the code with just one dataset and it works well. However, when I test it with the combined dataset, it is not the case. After some eyeball checking, it may be due to some occasional gaps within the list that are caused by some bugs on Zepa's part. To resolve this issue, I tried deleting the gaps manually, but the sample size is large, so that this method is quite inefficient.&lt;/p&gt;
&lt;p&gt;&lt;img alt="example of special cases" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_special-cases.png"&gt;&lt;/p&gt;
&lt;p&gt;Finally, I came up with a solution, which is to add a line of code: &lt;code&gt;match = re.match(r'^(.+?),\s*(\d+)$', line)&lt;/code&gt;, which can validate and extract the relevant information from each line. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;^&lt;/code&gt;: Anchors the pattern to the start of the string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(.+?)&lt;/code&gt;: Matches and captures one or more characters (except a newline) lazily. The &lt;code&gt;+&lt;/code&gt; indicates one or more occurrences, and the &lt;code&gt;?&lt;/code&gt; makes the matching lazy, meaning it captures as few characters as possible.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;,&lt;/code&gt;: Matches a comma character.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\s*&lt;/code&gt;: Matches zero or more whitespace characters. The &lt;code&gt;\s&lt;/code&gt; represents any whitespace character (spaces, tabs, etc.), and the &lt;code&gt;*&lt;/code&gt; indicates zero or more occurrences.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(\d+)&lt;/code&gt;: Matches and captures one or more digits. The &lt;code&gt;\d&lt;/code&gt; represents any digit character (0-9), and the &lt;code&gt;+&lt;/code&gt; indicates one or more occurrences.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&lt;/code&gt;: Anchors the pattern to the end of the string.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;re.match()&lt;/code&gt; function is used to check whether the line matches the specified pattern. If and only if a match is found, the code proceeds to extract the sentence and label. As a result, any lines with anomalies are ignored. However, two new issues have arisen. &lt;/p&gt;
&lt;p&gt;Firstly, there are some lines that match the specified pattern, but they do not actually represent the desired sample data and have been mistakenly included in the dataset. Fortunately, there are only a few lines with this issue, and it can be resolved manually.&lt;/p&gt;
&lt;p&gt;&lt;img alt="example of special cases 2" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_special-cases-2.png"&gt;&lt;/p&gt;
&lt;p&gt;Another issue is that the code can only read the lines after the gap in those gapped lists, as the lines before the gap do not match the expected format. There are numerous lines affected by this problem, and due to time constraints, I have decided to retain them in the dataset. I believe that including this noise data might potentially improve the training process. &lt;/p&gt;
&lt;p&gt;Nonetheless, if time permits, we aim to address the issue at its root cause, which is the data preprocessing stage. Our goal is to ensure the dataset is consistent and properly formatted, thereby eliminating the need to handle inconsistencies during the training process.&lt;/p&gt;
&lt;h2&gt;Web Scraping and Result Analysis (By Marcel, Wenkai, and Wanqian)&lt;/h2&gt;
&lt;p&gt;In order to get current text data to use for our sentiment analysis, we decided to scrape the &lt;a href="https://news.search.yahoo.com/search?p=search_example"&gt;Yahoo Finance search site&lt;/a&gt;. For that, we used a set of AI-related keywords.&lt;/p&gt;
&lt;p&gt;A few challenges arose during web scraping development:
- (presumably) temporary IP blocking
- duplicate data
- imprecise datetime values in text format.&lt;/p&gt;
&lt;p&gt;We started testing our scraper by running the scraping functions in a for loop for each keyword, trying to maximize AI-related news data. We noticed that the csv files we put the date in were alternating between having no data whatsoever and having (expected) hundreds to thousands of news entries. The pattern suggested a temporary inability to access the search site which we assumed to be due to temporary IP blocking. Our solution to this problem was to implement a timer that waits after each scraping iteration by keyword before starting the next iteration, which solved this issue. Considerations for a more sophisticated solution in the future are using proxy servers or rotating IPs and adjusting the timer dynamically based on the server’s response (minimizing the timer duration).&lt;/p&gt;
&lt;p&gt;As all keywords are related to each other (because we were using AI-related keywords) and because we were scraping every day to get the newest data, naturally, we expected duplicate data. To solve this we developed scripts that would parse the data of the different keywords and combine it into a combined file while eliminating duplicates, as well as add the newest distinct data to these files every day. &lt;/p&gt;
&lt;p&gt;Lastly, the search site did not return actual datetime values for the post date of the articles but returned text values such as “3 days ago”, “45 minutes ago”, and “1 week ago”. The higher the aggregation level of this time information, the more imprecise a conversation to a datetime value was. Example: If we have a data entry saying the post was released 45 minutes ago, we can parse it into a datetime value that is &lt;code&gt;now() - 45 minutes&lt;/code&gt;, meaning the exact value would only be off by seconds. 
However, for values where the date posted was “2 weeks ago”, &lt;code&gt;now() - 2 weeks&lt;/code&gt; could potentially be off by days, making these data points less helpful.&lt;/p&gt;
&lt;p&gt;When using the scraped data for analysis, the datetime imprecision problem got worse. The target AI index we analyzed was a US index, and we needed to further convert the time into Eastern Time. Therefore, we cannot guarantee that the news was published on that day. For example, we scraped data at 12:30 P.M. on April 20th and one piece of news showed “one day ago.” We then assumed the news was published at 12:30 P.M. on April 19th in Hong Kong time, and therefore, 12:30 A.M. on April 19th in  Eastern Time. But the news could be published at night on April 18th in Eastern Time.&lt;/p&gt;
&lt;p&gt;Despite the time imprecision, we also encountered data imbalance. The Yahoo Finance website tends to return more news published recently and less old news. Therefore, the sentiment scores may be less accurate for long ago dates due to inadequate data, while it takes more computing resources to produce sentiment scores for recent dates with thousands of pieces of news. However, this problem will diminish in the long run as we will collect more and more data.&lt;/p&gt;</content><category term="Reflection Report"></category><category term="Group Textonomy"></category></entry></feed>