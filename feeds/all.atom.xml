<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>FINA4350 Student Blog 2024</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/" rel="alternate"></link><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/feeds/all.atom.xml" rel="self"></link><id>https://buehlmaier.github.io/FINA4350-student-blog-2024-01/</id><updated>2024-03-04T22:30:00+08:00</updated><entry><title>Blog Post One - Initial Thoughts (by Group "Textonomy")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/blog-post-one-initial-thoughts-by-group-textonomy.html" rel="alternate"></link><published>2024-03-04T22:30:00+08:00</published><updated>2024-03-04T22:30:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/blog-post-one-initial-thoughts-by-group-textonomy.html</id><summary type="html">&lt;h2&gt;The Process We Come Up With Our Topic&lt;/h2&gt;
&lt;p&gt;Without a doubt, we are now in an Artificial Intelligence (AI) era. Within a few years, we went from traditional Deep Learning models to seemingly close to being near Artificial General Intelligence, and OpenAI just released Sora which amazed the world. With …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;The Process We Come Up With Our Topic&lt;/h2&gt;
&lt;p&gt;Without a doubt, we are now in an Artificial Intelligence (AI) era. Within a few years, we went from traditional Deep Learning models to seemingly close to being near Artificial General Intelligence, and OpenAI just released Sora which amazed the world. With the increasing popularity of AI, the demand for AI-related products has also increased, including Nvidia’s high-performance advanced graphics processing units (GPUs). The advanced chips have become a critical component in AI development, powering deep learning algorithms and accelerating computational tasks in different fields, ranging from healthcare and finance to manufacturing and transportation. As a result, Nvidia’s stock reached its historical peak of USD 823.94 on 23-02-2024 from USD 492.44 on 02-01-2024, a substantial increase of 67.32%, making it an attractive investment option for those seeking exposure to the flourishing AI sector.&lt;/p&gt;
&lt;p&gt;The recent phenomenal surge of demand in overall AI technology and Nvidia's stock price rise have inspired our group to apply text analytics and Natural Processing Language (NLP) techniques to analyze investors’ sentiment toward AI development and its influences on AI sector stocks.&lt;/p&gt;
&lt;p&gt;Our group believes the analysis of investors' sentiment toward AI development is important for understanding market dynamics and identifying trends that can potentially impact asset allocations and trading decisions. By leveraging text analytics techniques, such as sentiment analysis and opinion mining, our group aims to extract valuable financial insights from textual data sources, including news articles and social media posts, to analyze how positive or negative sentiment towards AI development and consider it as a systematic risk of AI sector stocks.&lt;/p&gt;
&lt;h2&gt;The Potential Ways to Achieve Our Goals&lt;/h2&gt;
&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;For our sentiment analysis project focusing on AI discourse, we aim to collect data from sources talking about financial information regarding AI-related companies such as news websites like &lt;a href="https://finance.yahoo.com/"&gt;Yahoo Finance&lt;/a&gt;, which provides extensive financial news coverage, including AI developments, or the Financial Times. To diversify our dataset, we consider platforms like &lt;a href="https://www.reddit.com/"&gt;Reddit&lt;/a&gt;, where community discussions can offer valuable insights into public sentiment.&lt;/p&gt;
&lt;p&gt;Our web scraping strategy involves tools such as &lt;a href="https://www.crummy.com/software/BeautifulSoup/"&gt;Beautiful Soup&lt;/a&gt; for parsing HTML/XML documents, Requests for HTTP requests, and &lt;a href="https://www.selenium.dev/"&gt;Selenium&lt;/a&gt; for dynamic content. Compliance with each platform's guidelines ensures ethical data collection. We'll organize the scraped data, including article headlines, publication dates, and texts, for preprocessing.&lt;/p&gt;
&lt;p&gt;The preprocessing phase is crucial for cleaning and standardizing the data. We plan to remove HTML tags, special characters, and whitespace, normalize text to lowercase, and apply tokenization, stop words removal, and lemmatization using libraries like &lt;a href="https://www.nltk.org/"&gt;NLTK&lt;/a&gt; and &lt;a href="https://spacy.io/"&gt;spaCy&lt;/a&gt;. These steps are designed to refine the dataset, making it suitable for sentiment analysis through iterative scripting and quality checks.&lt;/p&gt;
&lt;p&gt;For pretraining data, we have searched online and found some potentially useful labeled datasets, such as &lt;a href="https://github.com/zfz/twitter_corpus"&gt;Sanders&lt;/a&gt; (Twitter sentiment corpus), &lt;a href="https://ieee-dataport.org/open-access/stock-market-tweets-data"&gt;Taborda&lt;/a&gt; (stock market tweets data), &lt;a href="https://huggingface.co/datasets/financial_phrasebank"&gt;financial phrase-bank&lt;/a&gt; (financial news headings and sentiments), and &lt;a href="https://sites.google.com/view/fiqa/"&gt;FiQA&lt;/a&gt; (financial news, microblog messages, and their sentiments). They may have different data structures and label methods, and we will preprocess them to form a large uniform dataset for training, validation, and testing.&lt;/p&gt;
&lt;h3&gt;Model Selection &amp;amp; Training&lt;/h3&gt;
&lt;p&gt;Considering the limited time and computing resources, we plan to select a middle-size language model such as Bert, or FinBert (Bert with some financial knowledge pretraining). The goal is to train the model to take in texts and produce a sentiment score (e.g., 1 for very negative and 10 for very positive) with our collected pretraining datasets. Another potential way to train the model is to make use of large language models like ChatGPT and Bard. We can use the large ones as teachers producing fake training data and teach our middle-size model how to predict. In this way, we expect to get a model performing slightly worse than the large ones or on par with them. In the last stage, we will apply the trained Bert model to our collected real-life data from the Internet.&lt;/p&gt;
&lt;h3&gt;Potential Uses of Results&lt;/h3&gt;
&lt;p&gt;With the sentiment scores, we can get a general idea of the market sentiment toward AI development. For example, we may use the weighted average score of all texts analyzed as the overall market sentiment. We are then able to predict whether the opening price of AI sector stocks will rise or fall on the next day based on the overall sentiment on that day. In addition, we intend to find the correlation between the sentiment and the price change so that we can even predict how much will the price rise or fall. If time permits, we will also design trading strategies based on the results of the model predictions.&lt;/p&gt;
&lt;h2&gt;Expected Challenges During The Project&lt;/h2&gt;
&lt;p&gt;As now the project is in its early stages, we expect to encounter some challenges in detailed tasks. Firstly, the data sources we selected may have an anti-crawler mechanism (our project is for academic purposes only), which limits our ability to obtain large datasets. Secondly, the collected data may not be suitable for model training or prediction. Thirdly, how we weigh the scores across all texts needs to be discussed. Lastly, the correlation between the market sentiment toward AI development and the price of AI sector stocks may not be significant as there are also other factors influencing the market.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group Textonomy"></category></entry><entry><title>Project Introduction – Predicting Smartphone Sales in the United States (by Group "SalesEQ")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/project-introduction-predicting-smartphone-sales-in-the-united-states-by-group-saleseq.html" rel="alternate"></link><published>2024-03-04T18:00:00+08:00</published><updated>2024-03-04T18:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/project-introduction-predicting-smartphone-sales-in-the-united-states-by-group-saleseq.html</id><summary type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;p&gt;Zixian Fan, majoring in Statistics, with a second major in Finance, and a minor in Computer Science. Loves math, loves quantitative research, and has a sweet tooth. Loves to play games, with previously being in the Legends segment of Hearthstone.
&lt;a href="https://github.com/FanZixian"&gt;https://github.com/FanZixian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maximilian Droschl, Bachelor in …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;p&gt;Zixian Fan, majoring in Statistics, with a second major in Finance, and a minor in Computer Science. Loves math, loves quantitative research, and has a sweet tooth. Loves to play games, with previously being in the Legends segment of Hearthstone.
&lt;a href="https://github.com/FanZixian"&gt;https://github.com/FanZixian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maximilian Droschl, Bachelor in Economics with focus on Econometrics and Data Science, Exchange Student from the University of St. Gallen. Has a passion for chess and loves playing lacrosse. 
&lt;a href="https://github.com/MDrschl"&gt;https://github.com/MDrschl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ricky Choi, double degree in Computer Science and Finance at HKU. Loves teamsports, working out, and playing snooker. Also enjoys watching movies and animes. 
&lt;a href="https://github.com/Rickycmk"&gt;https://github.com/Rickycmk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jason Li, majoring in Quantitative Finance and minoring in Computer Science. Loves playing sports like volleyball and football. Also a lover of sitcoms and movies.
&lt;a href="https://github.com/Jasonlcyy"&gt;https://github.com/Jasonlcyy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mahir Faiaz, Bachelor of Arts and Sciences in Financial Technology. An avid international debater and a silly soccer lover.
&lt;a href="https://github.com/MahirFaiaz"&gt;https://github.com/MahirFaiaz&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Objective of this blog&lt;/h2&gt;
&lt;p&gt;This blogpost is intended to provide a surface-level idea about the ins and outs of our project. We will elaborate on the exact objective of our project, and then illustrate possible mechanisms to reach our end goals. As we do that, we will also be discussing the viability of our proposed methods and our targeted data sources.&lt;/p&gt;
&lt;p&gt;This blogpost is designed to allow the reader to get a good insight into the types of discussions we indulged in to get to this stage of the project. We believe such a practice will allow the reader to get a deeper understanding of our motivations and the ability to grasp what events led to a given outcome. In the long run, this should give the Professor a proper timeline of our project and an idea about the group atmosphere at any given point in the timeline, leading to the possibility of a more comprehensive evaluation.&lt;/p&gt;
&lt;h2&gt;Outline of the Project&lt;/h2&gt;
&lt;p&gt;The overarching objective of our project will be to predict smartphone sales in the United States. This is primarily motivated by the fact that the smartphone industry has become increasingly competitive in recent years, forcing manufacturers to develop new strategies. One of these strategies is forecasting industry sales, which, if mismanaged, can have significant consequences. The rapid pace of product development, increasing differentiation among smartphones, and relatively short life cycles of smartphones contribute to unpredictable sales patterns and increased volatility, exacerbating the challenge. Traditional models are mostly based on past values of the sales series itself, variables related to the product, such as its price or the brand, consumer sentiment indices, and economic variables such as the consumer price index or stock indices. However, we are motivated to extend these traditional techniques to a hybrid forecasting model that aims to incorporate sentiment indices estimated on the basis of text data related to phone sales. Thereby, we test the hypothesis whether sentiment scores derived from news articles add predictive information to traditional model specifications.&lt;/p&gt;
&lt;h2&gt;Modeling and data scraping&lt;/h2&gt;
&lt;p&gt;In general, our approach employs three different models. An LDA model is used to properly extract information from large amounts of text data with content related to smartphone sales in the United States. The final prediction is then made by an ARIMAX model, which has the advantage of being able to interpret the significance of each predictor variable included in the model, while at the same time allowing the modeling of non-stationary multivariate data. For comparison reasons, an LSTM model will be fitted to the data.&lt;/p&gt;
&lt;p&gt;Note that we haven’t decided which text data to be included in our research, because it is not determined what websites, articles, forums are available for web scraping or whether these data are easy to process and effective to our model. Even after narrowing down the scope, the reliability and relevance of these sources are not necessarily ensured. The source and the effective method to query reliable and relevant articles/tweets are problems that need to be sorted. Although, we would like to mention some text data candidates that might be applied in our text analysis:&lt;/p&gt;
&lt;p&gt;Seeking Alpha
Twitters
Some articles from free financial journals&lt;/p&gt;
&lt;p&gt;Additionally, as the data is not determined yet, we cannot give the name of our exogenous variables that will be applied to the ARIMAX/LSTM model. Hence, they are also not included in this blog post, but will be included in our following blogs soon as soon as we have fitted them to the data.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Farkhod, A., Abdusalomov, A., Makhmudov, F., &amp;amp; Cho, Y. I. (2021). LDA-Based Topic Modeling Sentiment Analysis Using Topic/Document/Sentence (TDS) Model. Applied Sciences, 11(23), 11091.&lt;/p&gt;
&lt;p&gt;Ali, T., Omar, B., &amp;amp; Soulaimane, K. (2022). Analyzing Tourism Reviews Using an LDA Topic-Based Sentiment Analysis Approach. MethodsX, 101894.&lt;/p&gt;
&lt;p&gt;Sa-Ngasoongsong, A., Bukkapatnam, S. T., Kim, J., Iyer, P. S., &amp;amp; Suresh, R. P. (2012). Multi-step sales forecasting in automotive industry based on structural relationship identification. International Journal of Production Economics, 140(2), 875-887.&lt;/p&gt;
&lt;p&gt;Hwang, S., Yoon, G., Baek, E., &amp;amp; Jeon, B. K. (2023). A Sales Forecasting Model for New-Released and Short-Term Product: A Case Study of Mobile Phones. Electronics, 12(15), 3256.&lt;/p&gt;
&lt;p&gt;Jadhav, T. (2020). Prediction of Cell Phone Sales from Online Reviews Using Text Mining. International Journal of Research in Engineering, Science and Management, 3(8), 214-218.&lt;/p&gt;
&lt;p&gt;Lassen, N. B., Madsen, R., &amp;amp; Vatrapu, R. (2014, September). Predicting iphone sales from iphone tweets. In 2014 IEEE 18th International Enterprise Distributed Object Computing Conference (pp. 81-90). IEEE.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group SalesEQ"></category></entry><entry><title>Setbacks from News Sentiment Analysis (by Group "NewPotential")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/setbacks-from-news-sentiment-analysis-by-group-newpotential.html" rel="alternate"></link><published>2024-03-04T16:59:00+08:00</published><updated>2024-03-04T16:59:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/setbacks-from-news-sentiment-analysis-by-group-newpotential.html</id><summary type="html">&lt;p&gt;Authors: &lt;/p&gt;
&lt;p&gt;Thong Pei Cheng (Ocean) &lt;/p&gt;
&lt;p&gt;Jang Jungmin &lt;/p&gt;
&lt;p&gt;Wong Nicole &lt;/p&gt;
&lt;p&gt;Yao Yi Tung &lt;/p&gt;
&lt;p&gt;Yeung Cheuk Hin &lt;/p&gt;
&lt;h2&gt;Introduction:&lt;/h2&gt;
&lt;p&gt;In today's rapidly evolving financial landscape, the fusion of technology and finance has become increasingly intertwined. As a diverse team with academic backgrounds in fintech and quantitative finance, coupled with varied working experiences ranging …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Authors: &lt;/p&gt;
&lt;p&gt;Thong Pei Cheng (Ocean) &lt;/p&gt;
&lt;p&gt;Jang Jungmin &lt;/p&gt;
&lt;p&gt;Wong Nicole &lt;/p&gt;
&lt;p&gt;Yao Yi Tung &lt;/p&gt;
&lt;p&gt;Yeung Cheuk Hin &lt;/p&gt;
&lt;h2&gt;Introduction:&lt;/h2&gt;
&lt;p&gt;In today's rapidly evolving financial landscape, the fusion of technology and finance has become increasingly intertwined. As a diverse team with academic backgrounds in fintech and quantitative finance, coupled with varied working experiences ranging from tech roles to private banking, we are united by our shared belief in the profound applications of NLP in the financial industry.  &lt;/p&gt;
&lt;p&gt;To kickstart our project, we have chosen to analyse news articles and determine the sentiment surrounding the assets (e.g. stocks, bonds, commodities, etc) mentioned within them. By extracting sentiment from these articles, we aim to identify whether the mentioned assets are likely to experience positive or negative sentiment in the market. This approach will enable us to gain a deeper understanding of how news impacts market dynamics and potentially uncover valuable insights for investors. &lt;/p&gt;
&lt;p&gt;In this blog post, we will document our learning journey, and describe how we have come to a consensus on exploring this topic. we will present our findings thus far, shedding light on the relationship between news sentiment and financial market performance. &lt;/p&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;Before we delve into our present proposal, it's important to look back and reflect on our past ideas that didn't quite make the cut. We initially brainstormed a few concepts, such as developing a chatbot or a ChatGPT tailored to the financial data of companies, predicting loan approval, and analyzing news to generate summaries. In the end, we gravitated towards analyzing news with a focus on sentiment (positive/negative), and whether it was name-based or asset-based. &lt;/p&gt;
&lt;p&gt;Firstly, we entertained the idea of feeding a chatbot or ChatGPT with company annual reports. We envisioned a system where users could ask questions or search for keywords, akin to a Google search, and the chatbot would produce reliable answers. For example, in the realm of financial regulations, users could input legal terms, and the chatbot, powered by Python ChatterBot, would provide specific answers, much like a digital attorney. However, we eventually dropped this idea, taking into account that adopting a more recent and reliable than ChatGPT-2 would incur considerable expense. This led us to brainstorm another idea that could potentially make a more meaningful impact on people’s lives. &lt;/p&gt;
&lt;p&gt;Thus, after our first idea, we came close to initiating a project centered on loan approval prediction. The idea was to forecast the probability of loan approval based on customer data, such as credit score, income, employment type, and the purpose of the loan. This would potentially facilitate a smoother and quicker guidance for customers, sparing them the hassle of visiting banks and enduring lengthy procedures. However, the issue we encountered was, first, the difficulty in sourcing personal information, specifically variables like credit score and income. While we did consider having customers provide their personal information via a Google form or survey, after discussing with our professor, we concluded that we should reconceptualize and aim for a topic that encompasses finance more broadly. Apart from that, loan approval may heavily focus more on numerical data, such as the credit score of an individual or total asset value that an entity holds. Text analytics may play a less important role in decision making, which weakened the significance of final predicted result. &lt;/p&gt;
&lt;p&gt;The closest idea to what we aim to achieve with everyday financial news was summarizing financial news articles on a daily basis driven by our group members' firsthand experience. Most of us had undergone the tedious process of manually summarizing news from 6 P.M. to 6 A.M. for morning updates during our internships.  Consequently, we thought that creating a tool to automate daily news updates could potentially alleviate the workload for interns in similar positions. However, we failed at fulfilling the element of finance in this news aggregator. So, these previous "failures" have informed and shaped our current direction: providing a quicker and better understanding to investors of diverse market trends by analyzing news with three different sentiments. &lt;/p&gt;
&lt;h2&gt;Why this project?&lt;/h2&gt;
&lt;p&gt;The world of finance and investing is highly dynamic, with asset prices influenced by a multitude of factors, including market trends, economic indicators, and company-specific news. Staying informed about these factors is crucial for making informed investment decisions. However, the sheer volume and complexity of news articles and reports can make it challenging for investors to quickly and accurately gauge the sentiment surrounding particular assets. &lt;/p&gt;
&lt;p&gt;To address this challenge, the development of a news sentiment analysis tool specifically tailored to follow the trend of a certain asset offers significant value. Such a tool would leverage advanced natural language processing and machine learning techniques to automatically monitor and analyze news articles, press releases, and other relevant sources of information about the asset. &lt;/p&gt;
&lt;p&gt;The primary motivation for creating this tool is to provide investors with real-time insights into the sentiment and market perception surrounding a specific asset mentioned in the news. By quantifying and analyzing the “positive”, “negative”, or “neutral” sentiment expressed in the news, investors can gain a better understanding of how the market is responding to various events and news releases related to the asset. &lt;/p&gt;
&lt;p&gt;This tool would enable investors to quickly identify trends, sentiment shifts, and potential market-moving events, empowering them to make more informed trading decisions. Additionally, it can help investors assess the impact of news sentiment on asset prices and uncover potential trading opportunities or risks. By automating the sentiment analysis process, this tool would also save investors valuable time and effort, freeing them to focus on other aspects of their investment strategies.  &lt;/p&gt;
&lt;p&gt;In a nutshell, the creation of a news sentiment analysis tool specifically designed to follow the news of a particular asset holds immense value for investors. It offers the potential to enhance decision-making, improve timing, and provide a deeper understanding of market sentiment, ultimately aiding investors in achieving their financial objectives. &lt;/p&gt;
&lt;h2&gt;Kickstart with code&lt;/h2&gt;
&lt;p&gt;Why not start with a little experiment, extracting a piece of news to understand the sentiment? Hence, the first step taken was web scrapping to collect textual data. Python modules like selenium to interact with chrome played an important role to locate our target article. Below is a sample of the applied program code: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Setup for selenium &lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.common.by&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;By&lt;/span&gt; 

&lt;span class="c1"&gt;# Prep work for opening Chrome browser &lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ChromeOptions&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 
&lt;span class="n"&gt;hrome_options&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--disable-notifications&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Chrome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# head to webpage of sample article &lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.cnbc.com/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html?&amp;amp;qsearchterm=bitcoin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# extract all textual content of the article to a variable &lt;/span&gt;
&lt;span class="n"&gt;article_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_elements&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;By&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;XPATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;//div[@class=&amp;#39;ArticleBody-articleBody&amp;#39;]/div[@class=&amp;#39;group&amp;#39;]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plain_txt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;article_text&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, by utilising the nltk package, an overall sentiment score was calculated, with the value of: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.sentiment.vader&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;SIA&lt;/span&gt; &lt;span class="c1"&gt;# put &amp;quot;positive&amp;quot;, &amp;quot;neutral&amp;quot;, &amp;quot;negative&amp;quot; label to pieces of text &lt;/span&gt;
&lt;span class="n"&gt;sia&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SIA&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# instantiate imported module &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plain_txt&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output: {'neg': 0.0, 'neu': 0.876, 'pos': 0.124, 'compound': 0.9799} &lt;/p&gt;
&lt;p&gt;The high neutral was out of our expectation as we were hoping to be able to identify the potential of a certain asset class after the machine read the news. Simply understanding the tone of news article will not be enough to generalise reliable advice. Hence, further actions should be taken while analysing the news, given that they tend to be objective and remain neutral. &lt;/p&gt;
&lt;h2&gt;Next Step&lt;/h2&gt;
&lt;p&gt;It is suspected that the machine didn’t understand financial content, as it only calculated the sentiment score by general tone of text. Referencing to a similar project conducted by the IMF (Puy, 2019), labels of “bullish” and “bearish” were used while financial-related positive and negative terms were sorted out. Therefore, more research will be conducted to formulate better code for sentiment analysis.  &lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pound, J. (2024, February 28). Bitcoin ETFs see record-high trading volumes as retail investors jump on Crypto Rally. CNBC. &lt;a href="https://www.cnbc.com/amp/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html"&gt;https://www.cnbc.com/amp/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Puy, D. (2019, December 16). The power of text: How news sentiment influences financial markets. IMF. &lt;a href="https://www.imf.org/en/Blogs/Articles/2019/12/16/blog-the-power-of-text"&gt;https://www.imf.org/en/Blogs/Articles/2019/12/16/blog-the-power-of-text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog Post"></category><category term="Group NewPotential"></category></entry><entry><title>Intro - ESG News and Stock Volatility (by Group "ESG &amp; Volatility")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/intro-esg-news-and-stock-volatility-by-group-esg-volatility.html" rel="alternate"></link><published>2024-03-04T00:00:00+08:00</published><updated>2024-03-04T00:00:00+08:00</updated><author><name>Group ESG &amp; Volatility</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/intro-esg-news-and-stock-volatility-by-group-esg-volatility.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In our first blog post, we will provide an introduction to our project and offer some insights into our preliminary data collection process.&lt;/p&gt;
&lt;p&gt;The aim of our project is to discover the relationship between ESG (Environmental, social, and governance) news coverage and the volatility of companies. We aim to …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In our first blog post, we will provide an introduction to our project and offer some insights into our preliminary data collection process.&lt;/p&gt;
&lt;p&gt;The aim of our project is to discover the relationship between ESG (Environmental, social, and governance) news coverage and the volatility of companies. We aim to look at different sources, such as major news providers or social media like Twitter, focusing on ESG-related keywords to conduct sentiment analysis to see whether the frequency of mentions affects the volatility of companies. For example, companies that have a higher frequency of negative ESG-related news may portray higher price volatility in the market. Ultimately, we aim to see the level of significance of ESG news in stock prices.&lt;/p&gt;
&lt;p&gt;We came up with this idea because of the growing acknowledgment of ESG factors in business and investment decisions today, and the impact of news on shaping investors’ behaviour in the market. Recently, there has been a surge of news on governance issues of Kakao Corp., one of the prominent growth stocks in Korea. Allegations of stock price manipulation and embezzlement have surfaced, leading to an immediate decline in Kakao's stock price. This example has sparked our motivation to delve deeper into the connection between ESG sentiment scores and their influence on investors and eventually, stock prices.&lt;/p&gt;
&lt;h2&gt;Literature Review&lt;/h2&gt;
&lt;p&gt;Here are some articles we referenced.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0313592623001601"&gt;The Effect of ESG performance on the stock market during the COVID-19 Pandemic — Evidence from Japan&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The methodology section of the article details the use of a firm fixed effects panel model to examine the impact of ESG (environmental, social, and governance) on the returns, volatility, and liquidity of the stocks of more than 300 listed companies in Japan. The study utilizes ESG scores from FTSE Russell and Standard &amp;amp; Poor's to fully assess ESG performance. The article provided us with a methodology on how to establish the link between ESG performance and company stock performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://towardsdatascience.com/spot-vs-sentiment-nlp-sentiment-scoring-in-the-spot-copper-market-492456b031b0"&gt;Trading Sentiment: NLP &amp;amp; Sentiment Scoring the Spot Copper Market&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main objective of the article is to analyze the sentiment behind each tweet and its correlation with the spot price of copper over the past five years from historical tweets posted by financial news publishers on Twitter by using natural language processing. Techniques include data acquisition using GetOldTweets3, Exploratory Data Analysis (EDA), text preprocessing using Stopwords, tokenization, n-grams, Stemming &amp;amp; lemmatization, using GenSim and NLTK PyLDAvis, through Latent Dirichlet Allocation (LDA) model to analyze the data, and sentiment scoring using VADER. The article provides us with information regarding how to extract the data and how to preprocess the text. It also reminds us about the importance of data validation, as data may be lost or valuable data may not be included in the preprocessing process due to improper use of methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://doi.org/10.1007/s12351-023-00745-1"&gt;ESG performance, herding behavior and stock market returns: evidence from Europe&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This research explores the dynamics between ESG performance, investor herding, and stock market outcomes in Europe, analyzing data from large-cap companies across six nations from 2010 to 2020. It assesses how ESG scores and financial metrics like market size, price-to-book value, and the Sharpe ratio influence stock behaviour. The study leverages panel data methodology and sources its ESG information from the Refinitiv Eikon database, offering insights into the financial effects of ESG practices and collective investment patterns in the European markets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://doi.org/10.3390/su14148745"&gt;Proposing an Integrated Approach to Analyzing ESG Data via Machine Learning and Deep Learning Algorithms&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The study discusses a framework that employs artificial intelligence for the analysis of ESG data, underscoring its pivotal role in making well-informed investment decisions, particularly highlighted by the COVID-19 crisis. It explores the application of machine learning and deep learning algorithms to enhance the precision in forecasting companies' ESG performance indicators, such as focusing on governance and social datasets through NLP algorithms. Through various experiments and methods, the research focuses on improving the predictability of ESG scores and the importance of safeguarding ESG data against potential security threats. &lt;/p&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;To perform an analysis of the relationship between ESG news and stock volatility, raw news data is required. We demonstrated a test on news data collection via web scraping.&lt;/p&gt;
&lt;p&gt;&lt;img alt="FinViz News Table" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_01_image-finviz-snap.png"&gt;&lt;/p&gt;
&lt;p&gt;We tried to scrape the news heading from FinViz (a stock screening website). First, we download the HTML for the specific stock and get the news table via requests and BeautifulSoup.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://finviz.com/quote.ashx?t=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;

  &lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;my-app&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lxml&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;news_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;news-table&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;news_table&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, we parse it into a data frame with three columns &lt;code&gt;[‘heading’, ‘datetime’, ‘source’]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[A-Z][a-z]+-\d+-\d+&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_time&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\d+:\d+[A-Z]M&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\(.*\)$&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_heading&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;heading&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;source&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ffill&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;source&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_heading&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The format of the output DataFrame is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Output DataFrame" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_01_image-df-snap.png"&gt;&lt;/p&gt;
&lt;p&gt;With a for loop, we can scrap the news headings stock-by-stock and save them into a dict indicated with the stock symbol.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;symbols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MSFT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AMZN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GOOGL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;META&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;NFLX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TSLA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;news&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;symbol&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;symbols&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parse_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;news&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This method allows us to collect data from nearly all well-structured HTML such as SeekingAlpha, Yahoo!News, etc. Yet, this will yield too much unnecessary information for the purpose of collecting ESG news, since it scrapes all available news presented on the website without any filtering. One possible solution may be clustering the news data by topic and letting the machine learning algorithms decide what ESG news is, but the work will require a fair amount of time and resources. Therefore, it may be better to filter the ESG news before collecting the data. For example, we may use the search engine or existing libraries to filter the news. In turn, this means we need to use some predefined ESG keywords to perform the search.&lt;/p&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;In our project, we faced significant data collection and analysis challenges. One of the main issues is the sheer volume of data available on news sites and social media platforms related to ESG topics mentioned about companies. The sheer volume of information poses potential problems for data collection, storage, processing, and analysis, so we must improve our data collection methods and focus on the most relevant information. Additionally, changes in social media platforms (e.g., Twitter changed to the X) created problems with data access and the potential for outdated or incomplete datasets. These updates require us to make timely adjustments to our data collection strategy to ensure continued access to up-to-date, comprehensive data. In reflecting on these challenges, we recognize the need to be flexible and innovative in our approach to data collection and analysis, which has allowed us to effectively overcome these barriers.&lt;/p&gt;
&lt;p&gt;Currently, we have identified another challenge that we need to address, which is how to determine if a news article or social media text contains ESG-related content. This is important to ensure that the data we collect for the program is useful. In the previous literature review, one of the studies used the FTSE Russell's ESG Scores and data model, which gives us a well-established catalogue of keywords. For example, for the environment component in ESG, we can look for keywords like “Biodiversity,” “Climate Change,” and “Pollution &amp;amp; Resources.” We are working towards writing code for this section step by step.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group ESG &amp; Volatility"></category></entry><entry><title>Ideation - What, How, Why, and Why Not (By Group "TheWay")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/ideation-what-how-why-and-why-not-by-group-theway.html" rel="alternate"></link><published>2024-03-03T14:00:00+08:00</published><updated>2024-03-03T14:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-03:/FINA4350-student-blog-2024-01/ideation-what-how-why-and-why-not-by-group-theway.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The very first step of this group project, as always, is to formulate ideas and check their feasibility. In this blog post, we would like to discuss about the few ideas that we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The very first step of this group project, as always, is to formulate ideas and check their feasibility. In this blog post, we would like to discuss about the few ideas that we have. More specifically, we discussed about what exactly the topic is about, how NLP could be involved, and the reasons why the proposed topic should and should not be chosen.&lt;/p&gt;
&lt;h2&gt;Idea 1: ESG Ratings Prediction&lt;/h2&gt;
&lt;h4&gt;ESG Rating and its Significance&lt;/h4&gt;
&lt;p&gt;The Environmental, Social, and Governance (ESG) score is often used to evaluate a company's strength of commitment to sustainability and responsible business practices. It helps socially responsible investors on making investment decisions by providing insights into the company's sustainability performance.&lt;/p&gt;
&lt;p&gt;While ESG rating agencies like MSCI, Sustainalytics and Bloomberg have different methodology, the following are generally considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environmental impact (greenhouse gass emissions, energy efficiency, etc.),&lt;/li&gt;
&lt;li&gt;Social responsibility (diversity and inclusion, product safety, etc.),&lt;/li&gt;
&lt;li&gt;Coperate governance practices (board composition and diversity, shareholder rights, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/whats-esg-score-how-calculated-koviid-sharma/"&gt;What is #ESG Score and How it is Calculated | LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://shorturl.at/tPQ37"&gt;How to Tell If a Company Has High ESG Scores&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Literature Review&lt;/h4&gt;
&lt;p&gt;A growing body of scholarly literature explores the application of natural language processing (NLP) techniques to extract structured data from ESG reports and subsequently analyze them using machine learning models. One such example is the &lt;a href="https://arxiv.org/html/2312.17264v1"&gt;ESGReveal&lt;/a&gt; methodology, which employs an LLM-based approach to harness NLP for the extraction of ESG data from corporate sustainability reports, ultimately generating ESG scores for companies.&lt;/p&gt;
&lt;p&gt;Furthermore, several pre-trained language models that have been finetuned to ESG-related tasks, such as &lt;a href="https://huggingface.co/nbroad/ESG-BERT"&gt;ESGBERT&lt;/a&gt;, are available. ESGBERT can be directly used for text classification/ topic identification in the ESG domain as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;

&lt;span class="c1"&gt;# load tokenizer and model&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbroad/ESG-BERT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbroad/ESG-BERT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# pipeline for text classification&lt;/span&gt;
&lt;span class="n"&gt;text_classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text-classification&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# perform text classification on a sample list of texts&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_classifier&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Our production line releases a lot of carbon dioxide.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Men are paid 3 times more than women.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From the above sample texts, the ESGBERT model detects the presence of the topic "GHG_Emissions" with a probability of 78.11% and "Labor_Practices" with a probability of 95.79%.&lt;/p&gt;
&lt;p&gt;To build a model that, instead of identifying topics from texts, determines the ESG score, one possible solution is to use the ESGBERT model as a base model and further employ transfer learning to finetune it with extra ESG rating data, such that it can capitalize on the knowledge acquired from extensive text data corpora to perform more specific tasks, i.e. to predict ESG ratings for companies.&lt;/p&gt;
&lt;p&gt;Another notable example in the literature is &lt;a href="https://huggingface.co/ai-lab/ESGify"&gt;ESGify&lt;/a&gt;, a machine learning model capable of predicting ESG scores for companies based on their financial data. This model employs a combination of financial ratios and machine learning algorithms to anticipate a company's ESG score.&lt;/p&gt;
&lt;h4&gt;Challenges and Research Value&lt;/h4&gt;
&lt;p&gt;If we go with this topic, there might be a lot of challenges ahead since ESG score is usually done and audited manually. We also keep in mind that ESG is a hot topic that will interfered with investor decision, thus this project can be useful when doing due diligence of a company to invest.&lt;/p&gt;
&lt;h2&gt;Idea 2: Congress Trading Analysis&lt;/h2&gt;
&lt;h4&gt;Background&lt;/h4&gt;
&lt;p&gt;Congress members have gained a huge financial success in the stock market. Many congress members have been out-performing the market for years, consistently. For example, Nancy Pelosi, is known as one of the best-performing fund managers, has a 65% return in the year 2023, beating the benchmark S&amp;amp;P (with a 24% upside) by 2.7 times. Below is a chart showing the return of Congress members VS SPY in 2023. You can see that Congress outperformed SPY a lot in 2023.&lt;/p&gt;
&lt;p&gt;&lt;img src='images/The-Way_01_congressVSspy.png' alt='Congress VS SPY' width='70%'&gt;&lt;/p&gt;
&lt;p&gt;(Image from &lt;a href="https://unusualwhales.com/politics/article/congress-trading-report-2023#tldr"&gt;Congress Trading Report 2023&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;References of similar works&lt;/h4&gt;
&lt;p&gt;By looking into social media and tweets from policy-makers, we may be able to gain some valuable investment insights or even make profits from their trading history. Some previous works could be referenced on this project topic:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/sa1K/Congressional-Stock-Trading/tree/main"&gt;Congress copy-trade Github repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.govinfo.gov/app/collection/crec/2024/01/01-02/3"&gt;Congress Debate History, can be used for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.govtrack.us/"&gt;Another website tracking Congress movement, could be used for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/unitedstates/congress"&gt;This is a community-run project to develop Python tools to collect data about the bills, amendments, roll call votes, and other core data about the U.S. Congress into simple-to-use structured data files.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/SpeakerPelosi"&gt;Twitter of Nancy Pelosi, contains tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sa1K/Congressional-Stock-Trading/tree/main"&gt;Uses Selenium to scrape and make transcations in robinhood based on weighted sum of all politician trades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datacoalition.org/"&gt;Contains Open-Data for US Government&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.capitoltrades.com/trades?per_page=96&amp;amp;politician=P000197#"&gt;Trace Congress party on their equity traded&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Code Skeleton&lt;/h4&gt;
&lt;p&gt;Below are the proposed procedures to process the data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Available datasets can be used directly or after being converted to texts using Optical Character Recognition (OCR). If data are insufficient, web-scraping with Selenium can be done as follows.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.chrome.options&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Options&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scrape&lt;/span&gt;

&lt;span class="c1"&gt;# setting up drivers&lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Options&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_experimental_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;detach&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Chrome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;trades&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# time.sleep(3.5)&lt;/span&gt;
    &lt;span class="n"&gt;trades2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scrape&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trade_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;https://www.capitoltrades.com/trades?per_page=96&amp;amp;page=&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;trades&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;trades&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trades2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ignore_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Write the scrapped content to .csv for further processing&lt;/li&gt;
&lt;li&gt;Data cleaning, dataset merging, data pre-processing&lt;/li&gt;
&lt;li&gt;Perform Natural Language Processing (NLP) model training and testing on the crawled textual data&lt;/li&gt;
&lt;li&gt;Signal generation: A classification problem -&amp;gt; [-1, 1], map it to specific market, perform Sentiment Analysis and provide confidence score [-1,1] to determine to SHORT/LONG the corresponding ETF&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Potential work): Correlation matrix to validate whether there's a correlation between Congress transaction history &amp;amp; the time they deliver the speeches&lt;/p&gt;
&lt;p&gt;OR passing certain acts &amp;amp; equities' price upstrike/downstrike, so we can determine who is the real "smart-money"&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Challenges&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Dataset pre-processing might not be easy as data sources are not same&lt;/li&gt;
&lt;li&gt;Twitter API is not completely free. Free users have limited access to the API functionalities.&lt;/li&gt;
&lt;li&gt;The usefulness of the signal: using NLP to analyse the speech of Congress might not be that useful, in a way that speeches are often delayed (the stock price usually change right after the speech/act is delivered/established). Nevertheless, the correlation matrix might help us distinguish who is the real smart-money, and then we can simply do copy-trade based on majority vote consensus on top-gainers (that has a high win-rate).&lt;/li&gt;
&lt;/ol&gt;</content><category term="Progress Report"></category><category term="Group TheWay"></category></entry><entry><title>Demo Blog Post</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/demo-blog-post.html" rel="alternate"></link><published>2022-01-31T01:12:00+08:00</published><updated>2022-01-31T01:12:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2022-01-31:/FINA4350-student-blog-2024-01/demo-blog-post.html</id><summary type="html">&lt;p&gt;By Group "Super NLP"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "Super NLP"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP and recalculated the return and 30 days volatility.&lt;/p&gt;
&lt;p&gt;The code we use is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;myvar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;DF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;XRP-data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;How to Include a Quote&lt;/h2&gt;
&lt;p&gt;As a famous hedge fund manager once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fed watching is a great tool to make money. I have been making all my
gazillions using this technique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How to Include an Image&lt;/h2&gt;
&lt;p&gt;Fed Chair Powell is working hard:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Powell" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-Fintech-Disruption_Powell.jpeg"&gt;&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group Super NLP"></category></entry></feed>