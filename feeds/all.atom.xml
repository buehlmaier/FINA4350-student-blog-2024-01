<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>FINA4350 Student Blog 2024</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/" rel="alternate"></link><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/feeds/all.atom.xml" rel="self"></link><id>https://buehlmaier.github.io/FINA4350-student-blog-2024-01/</id><updated>2024-04-30T22:30:00+08:00</updated><entry><title>Blog Post Two - Encounter, Analyze, and Solve (by Group "Textonomy")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/blog-post-two-encounter-analyze-and-solve-by-group-textonomy.html" rel="alternate"></link><published>2024-04-30T22:30:00+08:00</published><updated>2024-04-30T22:30:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-30:/FINA4350-student-blog-2024-01/blog-post-two-encounter-analyze-and-solve-by-group-textonomy.html</id><summary type="html">&lt;p&gt;As we are all new to NLP and related techniques, we believe sharing the challenges we met along the way is valuable. We hope these experiences can serve as a reference for future NLP learners.&lt;/p&gt;
&lt;h2&gt;Data Collection and Preprocessing (by Zepa)&lt;/h2&gt;
&lt;p&gt;In the data processing part for pre-training data, our …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we are all new to NLP and related techniques, we believe sharing the challenges we met along the way is valuable. We hope these experiences can serve as a reference for future NLP learners.&lt;/p&gt;
&lt;h2&gt;Data Collection and Preprocessing (by Zepa)&lt;/h2&gt;
&lt;p&gt;In the data processing part for pre-training data, our group has adopted data from 3 sources, namely &lt;strong&gt;Financial phrasebank&lt;/strong&gt; (financial news headings and sentiments), &lt;strong&gt;Sanders&lt;/strong&gt; (Twitter sentiment corpus), and &lt;strong&gt;Taborda&lt;/strong&gt; (stock market tweets data). &lt;/p&gt;
&lt;p&gt;One challenge here is that our group needs to select accurate, relevant, and useful data, and it will be best if those data are well-packed, cleaned, and tidied up already. Our group spent a certain amount of time selecting high-quality data input to ensure potentially high-quality output.&lt;/p&gt;
&lt;p&gt;Another little challenge here is that there may be an extra space at the last line of each individual processed sentence due to the usage of &lt;code&gt;\n&lt;/code&gt;, which is undesired, so our group modified the previous code of &lt;code&gt;code_taborada.py&lt;/code&gt; and utilized the &lt;code&gt;if-else&lt;/code&gt; statement to specifically make it excluded. Below is an example from &lt;code&gt;code_taborada.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="code snippet in code_taborada.py" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_code-snippet.png"&gt;&lt;/p&gt;
&lt;h2&gt;Model Training (By Bosco)&lt;/h2&gt;
&lt;p&gt;In this part, I will illustrate the challenges I face when training our BERT model with reference to the &lt;a href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert"&gt;"Classify text with BERT"&lt;/a&gt; tutorial on TensorFlow.&lt;/p&gt;
&lt;p&gt;The most challenging part is to read the dataset in python, which was collected and preprocessed by Zepa. Zepa combined the three datasets and transformed them into one consistent format, which consists of some lists of &lt;code&gt;[sentence, label]&lt;/code&gt;. The dataset is then exported as a &lt;code&gt;.txt&lt;/code&gt; file for later use. My job is to read the text file in Python and use the data to train the BERT model. Below is the code I used to read the lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;combined_result.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Remove leading and trailing whitespace and newline character&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c1"&gt;# Remove the square brackets&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Find the index of the last comma in the line&lt;/span&gt;
        &lt;span class="n"&gt;last_comma_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rindex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;last_comma_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;last_comma_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code assumes that each line in the text file is in the form of &lt;code&gt;[sentence, label]&lt;/code&gt;. It then proceeds to find the position of the last comma. Everything before the last comma is assigned to the sentence variable, and the integer after the last comma represents the label. One can easily extract the  i&lt;sup&gt;th&lt;/sup&gt; sentence by &lt;code&gt;data[i][0]&lt;/code&gt; and the  i&lt;sup&gt;th&lt;/sup&gt; label by &lt;code&gt;data[i][1]&lt;/code&gt;. The code functions effectively when the dataset is in a consistent format. I test the code with just one dataset and it works well. However, when I test it with the combined dataset, it is not the case. After some eyeball checking, it may be due to some occasional gaps within the list that are caused by some bugs on Zepa's part. To resolve this issue, I tried deleting the gaps manually, but the sample size is large, so that this method is quite inefficient.&lt;/p&gt;
&lt;p&gt;&lt;img alt="example of special cases" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_special-cases.png"&gt;&lt;/p&gt;
&lt;p&gt;Finally, I came up with a solution, which is to add a line of code: &lt;code&gt;match = re.match(r'^(.+?),\s*(\d+)$', line)&lt;/code&gt;, which can validate and extract the relevant information from each line. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;^&lt;/code&gt;: Anchors the pattern to the start of the string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(.+?)&lt;/code&gt;: Matches and captures one or more characters (except a newline) lazily. The &lt;code&gt;+&lt;/code&gt; indicates one or more occurrences, and the &lt;code&gt;?&lt;/code&gt; makes the matching lazy, meaning it captures as few characters as possible.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;,&lt;/code&gt;: Matches a comma character.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\s*&lt;/code&gt;: Matches zero or more whitespace characters. The &lt;code&gt;\s&lt;/code&gt; represents any whitespace character (spaces, tabs, etc.), and the &lt;code&gt;*&lt;/code&gt; indicates zero or more occurrences.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(\d+)&lt;/code&gt;: Matches and captures one or more digits. The &lt;code&gt;\d&lt;/code&gt; represents any digit character (0-9), and the &lt;code&gt;+&lt;/code&gt; indicates one or more occurrences.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&lt;/code&gt;: Anchors the pattern to the end of the string.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;re.match()&lt;/code&gt; function is used to check whether the line matches the specified pattern. If and only if a match is found, the code proceeds to extract the sentence and label. As a result, any lines with anomalies are ignored. However, two new issues have arisen. &lt;/p&gt;
&lt;p&gt;Firstly, there are some lines that match the specified pattern, but they do not actually represent the desired sample data and have been mistakenly included in the dataset. Fortunately, there are only a few lines with this issue, and it can be resolved manually.&lt;/p&gt;
&lt;p&gt;&lt;img alt="example of special cases 2" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/Textonomy_02_special-cases-2.png"&gt;&lt;/p&gt;
&lt;p&gt;Another issue is that the code can only read the lines after the gap in those gapped lists, as the lines before the gap do not match the expected format. There are numerous lines affected by this problem, and due to time constraints, I have decided to retain them in the dataset. I believe that including this noise data might potentially improve the training process. &lt;/p&gt;
&lt;p&gt;Nonetheless, if time permits, we aim to address the issue at its root cause, which is the data preprocessing stage. Our goal is to ensure the dataset is consistent and properly formatted, thereby eliminating the need to handle inconsistencies during the training process.&lt;/p&gt;
&lt;h2&gt;Web Scraping and Result Analysis (By Marcel, Wenkai, and Wanqian)&lt;/h2&gt;
&lt;p&gt;In order to get current text data to use for our sentiment analysis, we decided to scrape the &lt;a href="https://news.search.yahoo.com/search?p=search_example"&gt;Yahoo Finance search site&lt;/a&gt;. For that, we used a set of AI-related keywords.&lt;/p&gt;
&lt;p&gt;A few challenges arose during web scraping development:
- (presumably) temporary IP blocking
- duplicate data
- imprecise datetime values in text format.&lt;/p&gt;
&lt;p&gt;We started testing our scraper by running the scraping functions in a for loop for each keyword, trying to maximize AI-related news data. We noticed that the csv files we put the date in were alternating between having no data whatsoever and having (expected) hundreds to thousands of news entries. The pattern suggested a temporary inability to access the search site which we assumed to be due to temporary IP blocking. Our solution to this problem was to implement a timer that waits after each scraping iteration by keyword before starting the next iteration, which solved this issue. Considerations for a more sophisticated solution in the future are using proxy servers or rotating IPs and adjusting the timer dynamically based on the server’s response (minimizing the timer duration).&lt;/p&gt;
&lt;p&gt;As all keywords are related to each other (because we were using AI-related keywords) and because we were scraping every day to get the newest data, naturally, we expected duplicate data. To solve this we developed scripts that would parse the data of the different keywords and combine it into a combined file while eliminating duplicates, as well as add the newest distinct data to these files every day. &lt;/p&gt;
&lt;p&gt;Lastly, the search site did not return actual datetime values for the post date of the articles but returned text values such as “3 days ago”, “45 minutes ago”, and “1 week ago”. The higher the aggregation level of this time information, the more imprecise a conversation to a datetime value was. Example: If we have a data entry saying the post was released 45 minutes ago, we can parse it into a datetime value that is &lt;code&gt;now() - 45 minutes&lt;/code&gt;, meaning the exact value would only be off by seconds. 
However, for values where the date posted was “2 weeks ago”, &lt;code&gt;now() - 2 weeks&lt;/code&gt; could potentially be off by days, making these data points less helpful.&lt;/p&gt;
&lt;p&gt;When using the scraped data for analysis, the datetime imprecision problem got worse. The target AI index we analyzed was a US index, and we needed to further convert the time into Eastern Time. Therefore, we cannot guarantee that the news was published on that day. For example, we scraped data at 12:30 P.M. on April 20th and one piece of news showed “one day ago.” We then assumed the news was published at 12:30 P.M. on April 19th in Hong Kong time, and therefore, 12:30 A.M. on April 19th in  Eastern Time. But the news could be published at night on April 18th in Eastern Time.&lt;/p&gt;
&lt;p&gt;Despite the time imprecision, we also encountered data imbalance. The Yahoo Finance website tends to return more news published recently and less old news. Therefore, the sentiment scores may be less accurate for long ago dates due to inadequate data, while it takes more computing resources to produce sentiment scores for recent dates with thousands of pieces of news. However, this problem will diminish in the long run as we will collect more and more data.&lt;/p&gt;</content><category term="Reflection Report"></category><category term="Group Textonomy"></category></entry><entry><title>Review &amp; Reflection Part 1 - ESG News and Stock Volatility (by Group "ESG &amp; Volatility")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/review-reflection-part-1-esg-news-and-stock-volatility-by-group-esg-volatility.html" rel="alternate"></link><published>2024-04-30T19:12:00+08:00</published><updated>2024-04-30T19:12:00+08:00</updated><author><name>Group ESG &amp; Volatility</name></author><id>tag:buehlmaier.github.io,2024-04-30:/FINA4350-student-blog-2024-01/review-reflection-part-1-esg-news-and-stock-volatility-by-group-esg-volatility.html</id><summary type="html">&lt;p&gt;For this blog post, we will provide a cumulative review and reflection on our project, 'ESG &amp;amp; Volatility,' specifically on the Data Collection and Text Preprocessing and Cleaning portion of our project. &lt;/p&gt;
&lt;h2&gt;Data Collection:&lt;/h2&gt;
&lt;p&gt;Our project aims to explore how stock price volatility in a company reacts to positive and negative …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For this blog post, we will provide a cumulative review and reflection on our project, 'ESG &amp;amp; Volatility,' specifically on the Data Collection and Text Preprocessing and Cleaning portion of our project. &lt;/p&gt;
&lt;h2&gt;Data Collection:&lt;/h2&gt;
&lt;p&gt;Our project aims to explore how stock price volatility in a company reacts to positive and negative Environmental, Social, and Governance (ESG) news. Specifically, we want to assess whether stock prices are more sensitive to positive or negative ESG sentiments. To achieve this, we utilized Natural Language Processing (NLP) and text analytics to analyze the relationship between ESG news sentiment and stock price movements.&lt;/p&gt;
&lt;p&gt;Initially, selecting a focus company for our research was challenging. We reviewed over 20 companies listed on the NASDAQ (i.e., MSFT, AAPL) and S&amp;amp;P 500 (i.e., AMZN, GOOGL) examining their recent positive and negative events. Ultimately, we chose Starbucks Corp (SBUX) because it demonstrated a clear distinction between positive and negative ESG-related events, making it an ideal candidate for our study.&lt;/p&gt;
&lt;p&gt;We dedicated a significant portion of our project timeline to selecting a focus company, which caused us to accelerate the remaining tasks to meet our final deadline. Additionally, as beginner programmers, we spent considerable time learning to code while progressing through our project.&lt;/p&gt;
&lt;p&gt;Once we selected our focus company, we began compiling URLs of news articles related to positive and negative ESG events to scrape text data for sentiment analysis using Javascript and Python. This phase presented several challenges and required multiple iterations. Initially, we used BeautifulSoup to filter through the URLs, but it scraped all text from the news websites, not just the news content. As a result, we attempted to combine Selenium with BeautifulSoup to target only the news text but encountered numerous errors due to difficulties in integrating these two packages. Ultimately, after consulting with our professor, we decided to switch to Newspaper3k, a package specifically designed for scraping news text data, which proved more suitable for our project. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;links&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getElementsByTagName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;linksArr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[];&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;links&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;links&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;ping&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;links&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;href&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;includes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;google&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nx"&gt;linksArr&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;p&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;links&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;href&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;/p&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newWindowContent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;linksArr&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newWindow&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;window&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;open&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nx"&gt;newWindow&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newWindowContent&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;newWindow&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If we were to restart this process, it would be more beneficial to select news sources with a consistent format rather than using various outlets with different formats. Opting for articles from a single database, such as Factiva, would streamline the data collection and web scraping stages of our project. Additionally, during our initial attempt, we encountered several URLs with firewalls or subscription requirements that hindered our scraping efforts, as the packages we used were unable to bypass these barriers. A consistent format and source would also mitigate these issues, allowing for smoother data collection. &lt;/p&gt;
&lt;p&gt;Moreover, considering the large number of news article URLs we handled, it would have been more efficient to save these URLs in a CSV file. Parsing through the URLs from the CSV file would have allowed for a more organized and condensed overview, facilitating easier management and access during our project. However, we did manage to save the extracted details from each positive and negative ESG event into CSV files. This step was crucial for data preprocessing, allowing us to organize and analyze the information more effectively.&lt;/p&gt;
&lt;p&gt;Below is a snippet of our code used to handle the large number of news article URLs and extract details from each positive ESG event. We applied a similar process for the negative ESG events, effectively managing and analyzing data from both types of events:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;newspaper3k&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;newspaper&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Article&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;...&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_details&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;article&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Article&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;link&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;source&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;source_url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;publish_date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;publish_date&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;No Date Found&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Failed to process &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;


&lt;span class="c1"&gt;# List to store results&lt;/span&gt;
&lt;span class="n"&gt;news_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;


&lt;span class="c1"&gt;# Process each URL&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_details&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;news_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Save the results to a CSV file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;positive_news_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;csv_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;fieldnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;link&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;title&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;source&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DictWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fieldnames&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fieldnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeheader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Data saved to positive_news_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Text Preprocessing and Cleaning:&lt;/h2&gt;
&lt;p&gt;After we scraped the news articles, we moved on to text preprocessing. For text cleaning, we converted all text into lowercase and removed unnecessary non-word characters like punctuations in order to remove noise. Initially, we removed all punctuation marks; however, we discovered that this approach also removes the dash, combining the words that were separated by dashes into a single word. For example, 'company-operated' was combined into one word 'companyoperated' when the dash was removed, which has the potential to be misinterpreted when we run sentiment analysis. Therefore, we adjusted our code to remove all punctuation marks except for the dash as well as the exclamation mark since exclamation marks could convey sentiment. Below is the code snippet of our adjusted code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Define the clean_text() function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;clean_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;cleaned_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[^a-zA-Z\s!-]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;cleaned_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cleaned_text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cleaned_text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Yet, we still faced some limitations of the outcome and one example is that 'U.S.' was converted to 'us' when we removed the punctuations. This could completely change the meaning of the word and hence, limit the reliability of our results in subsequent stages of sentiment analysis. We also acknowledged some other limitations in our outcomes in further steps of text preprocessing. For instance, when we attempted stemming, we realized that stemming is not always perfect and can remove word components, leaving behind stems that are grammatically incorrect. Overall, our attempts in text preprocessing have taught us about the complexities involved in preparing for language analysis and how difficult it is to clean the text data perfectly, especially when dealing with a substantial volume of text data that cannot be manually cleaned individually. We also learned the importance of finding a balance between completeness and practicality in text preprocessing. While it is important to aim for the best possible data cleaning, we should also consider the feasibility and efficiency of the data cleaning methods being used. &lt;/p&gt;</content><category term="Progress Report"></category><category term="Group ESG &amp; Volatility"></category></entry><entry><title>ESG Risk Levels and Score Prediction Using LLM (By "TheWay")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/esg-risk-levels-and-score-prediction-using-llm-by-theway.html" rel="alternate"></link><published>2024-04-30T14:00:00+08:00</published><updated>2024-04-30T14:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-30:/FINA4350-student-blog-2024-01/esg-risk-levels-and-score-prediction-using-llm-by-theway.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In today's rapidly evolving investment landscape, Environmental, Social, and Governance (ESG) factors have gained significant recognition as key drivers of long-term value creation. According to Bloomberg Intelligence, a staggering 85% of investors agree …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In today's rapidly evolving investment landscape, Environmental, Social, and Governance (ESG) factors have gained significant recognition as key drivers of long-term value creation. According to Bloomberg Intelligence, a staggering 85% of investors agree that incorporating ESG considerations can lead to superior returns, bolster portfolio resilience, and enhance fundamental analysis. Projections indicate that by 2030, ESG investing is expected to account for a substantial portion, approximately US$40 trillion or 25%, of the total projected assets under management worldwide. Recognising the significance of ESG in investment decision-making, we have embarked upon the development of an ESG risk levels predictor, aimed at assisting investors in making informed and responsible investment choices.&lt;/p&gt;
&lt;h2&gt;1. Background Information&lt;/h2&gt;
&lt;h3&gt;1.1. What are ESG scores and ESG risk levels?&lt;/h3&gt;
&lt;p&gt;ESG scores and ESG risk levels are two distinct measures used to assess environmental, social, and governance (ESG) factors. ESG scores provide a comprehensive evaluation of a company's overall sustainability and responsible business practices, considering a wide range of ESG criteria. These scores reflect the company's commitment to sustainability and its ability to manage ESG risks and opportunities effectively. On the other hand, ESG risk levels focus specifically on quantifying the potential risks associated with a company's ESG practices, assessing factors such as carbon emissions, labor practices, and regulatory compliance. There are 5 ESG risk levels: “Negligible”, “Low”, “Medium”, “High” and “Severe”, which provide investors with an assessment of the potential downside risks. Due to this different objective, ESG score is not always correlate with ESG Risk Level. &lt;/p&gt;
&lt;h3&gt;1.2. The Significance of ESG Scores and ESG Risk Levels&lt;/h3&gt;
&lt;p&gt;Fund managers seeking to align their investment strategies with ESG principles often rely on ESG scores and risk level to identify potential investments for inclusion in ESG-focused exchange-traded funds (ETFs) and other sustainable investment products. Our ESG scores and risk levels predictor aims to leverage advanced data analytics and machine learning techniques to predict and generate accurate ESG scores and risk level for companies, empowering investors with valuable information for their investment decision-making processes.&lt;/p&gt;
&lt;h3&gt;1.3. Benefits of ESG Risk Levels and ESG Score Predictor&lt;/h3&gt;
&lt;p&gt;ESG risk levels and score predictors hold significant potential to revolutionise the investment landscape by providing investors with a reliable tool to evaluate the sustainability performance of companies. By harnessing the power of data analytics and machine learning algorithms, our predictor offers the following benefits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Enhanced Investment Decision-Making: Investors can make more informed choices by incorporating ESG scores into their investment analysis. Our predictor enables investors to identify socially responsible companies with strong sustainability practices, aligning their investments with their values.&lt;/li&gt;
&lt;li&gt;Risk Mitigation: By evaluating a company's environmental and social practices, investors can identify potential risks, such as regulatory non-compliance or reputational damage. Our predictor assists in identifying companies with robust governance frameworks, thereby reducing investment risks.&lt;/li&gt;
&lt;li&gt;Sustainable Impact: Through the promotion of ESG investing, our predictor contributes to the broader goal of driving positive change in corporate behaviour. By rewarding companies with strong ESG practices, investors can incentivise sustainable practices and foster a more responsible business environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, we decided to develop models to predict ESG Risk Levels and ESG Score as our project.&lt;/p&gt;
&lt;h2&gt;2. Data Gathering and Pre-processing&lt;/h2&gt;
&lt;p&gt;We gather data from two different source. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/code/mauriciovellasquez/esg-risk-analysis-insights-from-s-p-500-companies"&gt;Table consist of ESG Risk Level and Score&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts"&gt;Table consist of Earnings Call Transcript&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Firstly, we proceed by merging the data based on the Ticker and removing any rows that contain invalid or empty data before 2022. Subsequently, we eliminate the Q&amp;amp;A section from the transcript to reduce its length. In order to further condense the transcript for analysis purposes, we employ ESG-BERT to classify sections that are relevant to environmental, social, and governance (ESG) topics. This process results in an average word count reduction from 3012 to 1298. Additionally, we employ stop-word removal and lemmatization techniques before splitting the data into test and train sets.&lt;/p&gt;
&lt;p&gt;Upon examining the ESG score chart, we observe a left-skewed distribution, indicating that a larger number of companies possess lower ESG scores.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data Distribution" src="images/TheWay_02_Img01_TestTrainDataDist.png" title="Data Distribution"&gt;&lt;/p&gt;
&lt;p&gt;This data imbalance can introduce bias towards the majority class in our model. To address this concern, we employ class weighting techniques during the model development phase to mitigate the impact of this imbalance.&lt;/p&gt;
&lt;h2&gt;3. Development with Different Base Model&lt;/h2&gt;
&lt;h3&gt;3.1. ESG-Bert Fine-tuning&lt;/h3&gt;
&lt;h4&gt;3.1.1. Motivation&lt;/h4&gt;
&lt;p&gt;In this project, we actually focused on the BERT model for predicting ESG scores and risk levels.&lt;/p&gt;
&lt;p&gt;One of the main advantages of using BERT is its relatively lower computational resource requirements compared to other well-known powerful models like ChatGPT and Llama. BERT's lightweight nature allows us to achieve optimal performance even with medium-range GPUs. In contrast, getting the full potential of ChatGPT or Llama would require significantly more resources, which is not feasible for us. This accessibility and efficient use of limited resources make BERT a more practical choice for us.&lt;/p&gt;
&lt;p&gt;Besides, BERT's architecture is particularly well-suited for classification tasks, such as predicting ESG risk levels. It allows for straightforward application to classification problems, like predicting ESG risk levels in this project. In contrast, generative models like ChatGPT and Llama, despite their impressive capabilities, can be less intuitive for classification tasks due to their autoregressive nature.&lt;/p&gt;
&lt;p&gt;Another reason to choose BERT is its proven effectiveness in a variety of NLP tasks. As an older model compared to Llama or ChatGPT (BERT was published in 2018), BERT has been widely adopted and successfully applied across numerous research projects, giving us confidence in its ability to deliver strong results for ESG prediction tasks.&lt;/p&gt;
&lt;p&gt;Furthermore, there are already fine-tuned models specifically designed for ESG-related tasks. These models demonstrate the potential of BERT for addressing the unique challenges associated with predicting ESG scores and risk levels.&lt;/p&gt;
&lt;p&gt;ESG-Bert is one of the fine-tuned model available online. It is a &lt;strong&gt;Classification&lt;/strong&gt; Natural Language Processing model fine-tuned for dealing with ESG-related data &lt;strong&gt;classification&lt;/strong&gt; tasks. We are trying to fine-tune this model with our dataset for predicting the ESG scores and ESG risk level. We believe this approach would yield the best results.&lt;/p&gt;
&lt;h4&gt;3.1.2. Work Done&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nbroad/ESG-BERT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ignore_mismatched_sizes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We load the pre-trained ESG-Bert model and set its classifier as above for classifying the ESG risk level. On the other hand, if we would like to predict ESG scores, we have to replace the classification head with a linear layer to a continuous value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nbroad/ESG-BERT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# For regression, we need to remove the classification head that outputs logits for classes&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We then proceed to fine-tune. As mentioned, we have to deal with the data imbalance case.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;class_weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;esg_risk_level&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value_counts&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;iin&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;# Explicitly define as floatclass_weights= class_weights/ class_weights.sum()* num_classes&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nbroad/ESG-BERT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ignore_mismatched_sizes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdamW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;5e-5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;class_weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="c1"&gt;# Ensure weights are on the correct device and float&lt;/span&gt;

&lt;span class="n"&gt;train_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Use &amp;quot;cuda&amp;quot; if you have GPU&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Training Loop&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training Epoch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;input_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_ids&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;attention_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;attention_mask&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attention_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;attention_mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Ensure logits are FloatTensor&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;3.1.3. Model Setup With Class Weights to Handle Imbalance Data&lt;/h4&gt;
&lt;p&gt;To handle the imbalance, we calculated class weights for each ESG risk level. This approach helps the model treat each risk level fairly during the training process by giving more importance to less frequent classes. Essentially, we're telling the model that it's just as crucial to learn from the rarer ESG risk levels as it is from the more common ones.&lt;/p&gt;
&lt;p&gt;Another important aspect of our approach is the choice of the CrossEntropyLoss function. This loss function is specifically designed for multi-class classification problems, making it a suitable choice for predicting ESG risk levels. By incorporating class weights into the loss function, we further ensure that the model gives equal attention to all ESG risk levels during training, leading to a more accurate and well-rounded prediction capability.&lt;/p&gt;
&lt;h4&gt;3.1.4. Findings&lt;/h4&gt;
&lt;p&gt;Below is the performance of the fine-tuned ESG-Bert model. We also try the performance of the vanilla ESG-Bert model (without any fine-tuning) on our dataset to set a benchmark.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ESG risk level prediction accuracy&lt;/th&gt;
&lt;th&gt;Vanilla ESG-Bert&lt;/th&gt;
&lt;th&gt;Fine-tuned ESG-Bert&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Training data&lt;/td&gt;
&lt;td&gt;27.8%&lt;/td&gt;
&lt;td&gt;51.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Testing data&lt;/td&gt;
&lt;td&gt;21.7%&lt;/td&gt;
&lt;td&gt;46.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ESG score prediction on testing data&lt;/th&gt;
&lt;th&gt;Vanilla ESG-Bert&lt;/th&gt;
&lt;th&gt;Fine-tuned ESG-Bert&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MSE (Mean Squared Error)&lt;/td&gt;
&lt;td&gt;452.555&lt;/td&gt;
&lt;td&gt;62.933&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MAE (Mean Absolute Error)&lt;/td&gt;
&lt;td&gt;20.134&lt;/td&gt;
&lt;td&gt;20.134&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see an obvious improvement after fine-tuning, but the performance is a bit below our expectations. Despite the current accuracy not being exceptionally high, we believe the fine-tuned ESG-BERT model still holds a certain level of usefulness. The improvements over the vanilla model indicate that it has learned to adapt to our specific dataset and task. As we continue to refine our approach, with BERT or ESG-Bert, we expect the model's performance to improve, making it a valuable tool for predicting ESG risk levels and scores in the future.&lt;/p&gt;
&lt;h3&gt;3.2. Llama-2 Fine-tuning&lt;/h3&gt;
&lt;h4&gt;3.2.1. Motivation&lt;/h4&gt;
&lt;p&gt;Llama2 is a collection of pre-trained and fine-tuned large language models developed by Meta AI and released in 2023. These models are freely available for both research and commercial use. Llama2 AI models possess the ability to perform diverse natural language processing tasks, ranging from text generation to programming code analysis.  &lt;strong&gt;Llama2 is one of the strong LLM models, has been in used for fine-tuning for a while and is widely acknowledged to be even more powerful than BERT in general.&lt;/strong&gt; Therefore, we believe that utilising this model will yield significant results and valuable comparisons. As such, it is worthwhile to provide a more detailed account of our methods and the work we have undertaken.&lt;/p&gt;
&lt;p&gt;We decided to use &lt;em&gt;Llama-2-7b-chat-hf&lt;/em&gt;, the Llama-2 model with 7 billion parameters from HuggingFace for fine-tuning. There are versions with 13b and 70b parameters, which are much more powerful. However, since we have limited resources (GPU), we have to use this smaller model to try and compute everything in a reasonable runtime.&lt;/p&gt;
&lt;h4&gt;3.2.2. Work Done&lt;/h4&gt;
&lt;p&gt;To access the Llama-2 model, we have to do the following:&lt;/p&gt;
&lt;p&gt;First, visit https://llama.meta.com/llama-downloads/ and request access to Meta Llama with our email address. After receiving the permission email, log in to Hugging Face account on the Llama-2 model page at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf, and request access to the model repository.&lt;/p&gt;
&lt;p&gt;Once gained access to the model repository, set up the development environment by installing the Hugging Face Hub library. Run &lt;code&gt;pip install huggingface-hub&lt;/code&gt; in the environment and authenticate the registered account by running &lt;code&gt;huggingface-cli login&lt;/code&gt;. This will your account and grant you access to the Llama-2 model.&lt;/p&gt;
&lt;p&gt;With these steps completed, we can finally access the model with this line of code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;base_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meta-llama/Llama-2-7b-chat-hf&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We fine-tuned the Llama-2 model by the following approach:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;combine_question_answer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;transcript_esg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;esg_risk_level&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;system_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; \
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;You are an rating agency. Your task is to predict the a company&amp;#39;s ESG Risk Level from a meeting transcript.&amp;quot;&lt;/span&gt; \
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;You should evaluate the company&amp;#39;s performance on Environmental, Social and Governance issues.&amp;quot;&lt;/span&gt; \
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The possible Risk Levels, from low to high, are `Negligible`, `Low`, `Medium`, `High`, `Severe`.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; \
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;s&amp;gt;[INST] &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;system_msg&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;###Transcript: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;transcript_esg&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;###Risk Level: [/INST] `&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;esg_risk_level&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;`&amp;lt;/s&amp;gt;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prompt&lt;/span&gt;

&lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;combine_question_answer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transcript_esg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;esg_risk_level&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The purpose of creating this prompt is to make it super easy for our model to understand its mission. It's like giving it a treasure map and a set of instructions to follow.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;combine_question_answer&lt;/code&gt; function is like a magical spell that mixes three essential ingredients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;System message&lt;/strong&gt;: A friendly note that tells our model what it's supposed to do. It's like saying, "Hey buddy, you're a rating agency, and you need to figure out a company's ESG Risk Level from this meeting transcript. Oh, and here are the possible Risk Levels you can choose from!"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transcript&lt;/strong&gt;: This is the treasure map! It's the meeting transcript that our language model have to use to complete its mission.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk Level&lt;/strong&gt;: This is the final destination, the X on the treasure map. It's where our model should place the predicted ESG Risk Level.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By creating this fun and structured prompt, we're helping our language model learn more effectively and become a pro at predicting ESG Risk Levels from meeting transcripts. &lt;/p&gt;
&lt;p&gt;We then utilised the QLoRA (Quantised Low Rank Adaptation) method for fine-tuning our model. This approach is a type of PEFT (Parameter Efficient Fine Tuning) technique, which focuses on minimising the number of trainable parameters within a neural network. By optimising memory usage, QLoRA significantly reduces the time required for training. This time-saving advantage enables us to advance our project efficiently, even under tight time constraints.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;peft_params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LoraConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;lora_alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;lora_dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;task_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;CAUSAL_LM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By fine-tuning the Llama-2 model in this manner, we can effectively compare its performance with that of our main project focus, the ESG-BERT model, to determine the more accurate and efficient solution for assessing ESG risk levels.&lt;/p&gt;
&lt;h4&gt;3.2.3. Findings&lt;/h4&gt;
&lt;p&gt;The training and testing accuracies achieved by the fine-tuned Llama-2 model stand at 40.3% and 29.5%, respectively. The ESG-BERT model has demonstrated superior performance with an exact same dataset. This is likely because we are using the "weakest" Llama-2 model, with the least number of parameters, due to our limited computational resources. This constraint creates a bottleneck, preventing us from further improving the model's performance. &lt;strong&gt;As a result, we decided not to proceed with ESG score prediction and stopped at this point.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;3.3. BERT Fine-tuning&lt;/h3&gt;
&lt;h4&gt;3.3.1. Motivation&lt;/h4&gt;
&lt;p&gt;Since the performance of ESG-BERT is not yet optimal, and we believe that BERT models have the potential to yield good results, we have also decided to directly utilise BERT and fine-tune it for this project. &lt;/p&gt;
&lt;h4&gt;3.3.2. Work done&lt;/h4&gt;
&lt;p&gt;Here are the codes and parameters that we use.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;BASE_MODEL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;google-bert/bert-base-uncased&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-4&lt;/span&gt;
&lt;span class="n"&gt;MAX_LENGTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;
&lt;span class="n"&gt;EPOCHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BASE_MODEL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BASE_MODEL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;training_args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TrainingArguments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;output_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;checkpoints/bert_finetuned_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;per_device_train_batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;per_device_eval_batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_train_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EPOCHS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;evaluation_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;epoch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logging_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;epoch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;save_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;epoch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="c1"&gt;# metric_for_best_model=&amp;quot;mse&amp;quot;,&lt;/span&gt;
    &lt;span class="n"&gt;greater_is_better&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;save_total_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;load_best_model_at_end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;weight_decay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;report_to&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dataset_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eval_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dataset_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;compute_metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;compute_metrics_for_regression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filterwarnings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ignore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The work flow is basically the same as how we fine-tune the ESG-BERT model. &lt;/p&gt;
&lt;h4&gt;3.3.3. Findings&lt;/h4&gt;
&lt;p&gt;After training the model, here is the result of the ESG Score and Risk Level predictor model in train and test dataset. On the best case scenario, &lt;strong&gt;we manage to train a model with accuracy rate of above 75% on train data and above 52% in test data, which is higher than random guess and ESG-BERT fine-tuning that only have 51% accuracy for ESG Risk Level.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;We also noticed that the ESG score predictor is capped at 26. This might be due to the class imbalance that resulting in ESG score prediction by our model less likely to give score of higher than 26. With this, &lt;strong&gt;we calculate that our model can achieve MSE at 16.3790 and 27.7570&lt;/strong&gt; for training and testing dataset respectively, &lt;strong&gt;which is also better than ESG-BERT fine-tuning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Train Data Prediction" src="images/TheWay_02_Img02_TrainDataPred.png" title="Train Prediction Result"&gt;
&lt;img alt="Test Data Orediction" src="images/TheWay_02_Img03_TestDataPred.png" title="Test Prediction Result"&gt;&lt;/p&gt;
&lt;h2&gt;4. Deployment&lt;/h2&gt;
&lt;p&gt;Since we think that deploying the model to visualise the result would make the presentation more appealing than plain code, we deployed our model using Python Flask. &lt;/p&gt;
&lt;p&gt;At first, the deployment model do not have a very friendly GUI. It contains only one input field and returns a JSON string after &lt;code&gt;Search&lt;/code&gt; is clicked. &lt;/p&gt;
&lt;h3&gt;4.1. Original GUI&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Original GUI" src="images/TheWay_02_Img04_OriGUI.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Original GUI" src="images/TheWay_02_Img05_OriGUI.png"&gt;&lt;/p&gt;
&lt;h3&gt;4.2. What we want to achieve&lt;/h3&gt;
&lt;p&gt;We have had some discussions and think that a GUI that looked like this would look better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Design" src="images/TheWay_02_Img06_GUI.png"&gt;&lt;/p&gt;
&lt;h3&gt;4.3. Leveraging GenAI&lt;/h3&gt;
&lt;p&gt;So we put both our code + the image above into ChatGPT, and asked how we could use Bootstrap to create a GUI like the image as shown above. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Conversation with ChatGPT&lt;/p&gt;
&lt;p&gt;&lt;img alt="ChatGPT" src="images/TheWay_02_Img07_Chat.png"&gt;
&lt;img alt="ChatGPT" src="images/TheWay_02_Img08_Chat.png"&gt;
&lt;img alt="ChatGPT" src="images/TheWay_02_Img09_Chat.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After prompt engineering and finetuning the code provided, we refined the GUI. Also, we added the “Download Data” function so that users can download the predicted esg risk level, esg score, and ESG-related transcript into a .txt file for further processing. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Deploy GUI" src="images/TheWay_02_Img10_Deploy.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run the python server &lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;python ./{directory}/deploy_model.py 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Open the HTML file &lt;code&gt;index.html&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Deployment Details" src="images/TheWay_02_Img11_Deploy.png"&gt;&lt;/p&gt;
&lt;p&gt;Below is the workflow of the deployment: &lt;/p&gt;
&lt;p&gt;Case I: User enters a stock symbol in the HTML form and clicked &lt;code&gt;Search&lt;/code&gt;: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;JavaScript Handling:
Captures the form submission and prevents default submission, sends a AJAX request with the symbol to Flask server&lt;/li&gt;
&lt;li&gt;Python (Flask) Processing:
Receives AJAX request -&amp;gt; Searches in our predicted risk level.csv (which is the saved output result from the previous process)  -&amp;gt; Sends back response (ESG data or error message) -&amp;gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;JavaScript Updates HTML:&lt;/p&gt;
&lt;p&gt;Receives response from Flask server -&amp;gt; Updates HTML to display ESG data or an error message -&amp;gt; If data received, shows download button &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Case II: User clicks the &lt;code&gt;Download&lt;/code&gt; : &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;User clicks download button -&amp;gt; JavaScript triggers file download&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;5. Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we discuss our findings and experiences in developing a good model to predict ESG risk levels by fine-tuning ESG-BERT, Llama2 and BERT. We discovered that directly fine-tuning the BERT model yielded even more promising results than fine-tuning the ESG-BERT model, which was initially expected to perform better on ESG-related tasks. As a result, we suggest focusing on this direction in future work.&lt;/p&gt;
&lt;p&gt;And to make the presentation more appealing and showcase the results effectively, we deployed our model using Python Flask. This approach allowed us to visualise the results in a more interactive and user-friendly manner compared to plain code.&lt;/p&gt;
&lt;p&gt;One challenge we faced was the limited performance of the Llama2 version feasible to us. Computational resources proved to be a significant issue when working on this project.&lt;/p&gt;
&lt;p&gt;For future work, we aim to improve the accuracy of our model using BERT as the base model. To achieve this, we propose the following enhancements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expanding the dataset to include more companies and years of transcripts&lt;/li&gt;
&lt;li&gt;Incorporating additional data sources such as news articles and media coverage&lt;/li&gt;
&lt;li&gt;Conducting advanced feature engineering to explore and create additional relevant features&lt;/li&gt;
&lt;li&gt;Leveraging domain knowledge and external data sources to enrich the feature set&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By implementing these improvements, we hope to develop a more robust and accurate ESG risk level prediction model, ultimately providing investors with a valuable tool for evaluating companies' sustainability performance.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group TheWay"></category></entry><entry><title>Review &amp; Reflection Part 2 - ESG News and Stock Volatility (by Group "ESG &amp; Volatility")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/review-reflection-part-2-esg-news-and-stock-volatility-by-group-esg-volatility.html" rel="alternate"></link><published>2024-04-29T22:07:00+08:00</published><updated>2024-04-29T22:07:00+08:00</updated><author><name>Group ESG &amp; Volatility</name></author><id>tag:buehlmaier.github.io,2024-04-29:/FINA4350-student-blog-2024-01/review-reflection-part-2-esg-news-and-stock-volatility-by-group-esg-volatility.html</id><summary type="html">&lt;p&gt;For this blog post, we will provide a cumulative review and reflection on our project, 'ESG &amp;amp; Volatility,' specifically on the Sentiment Analysis and Stock Volatility portion of our project. &lt;/p&gt;
&lt;h2&gt;Sentiment Analysis:&lt;/h2&gt;
&lt;p&gt;Using the cleaned text data, we then conducted sentiment analysis on the news articles by using Vader from NLTK …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For this blog post, we will provide a cumulative review and reflection on our project, 'ESG &amp;amp; Volatility,' specifically on the Sentiment Analysis and Stock Volatility portion of our project. &lt;/p&gt;
&lt;h2&gt;Sentiment Analysis:&lt;/h2&gt;
&lt;p&gt;Using the cleaned text data, we then conducted sentiment analysis on the news articles by using Vader from NLTK. Our plan was to calculate the sentiment score for each date in order to match them with the daily volatility data later on in our process. Therefore, the articles were sorted into ascending order of dates. However, we realized that the number of news articles that were scraped varied for each date. First, we attempted to summarize the news sentiment of each date by determining the average sentiment score for every article. Below shows the code snippet of our initial attempt: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Open the cleaned_data CSV file&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;final_data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Calculate sentiment scores for each article&lt;/span&gt;
&lt;span class="n"&gt;sentiment_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;analyzer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
   &lt;span class="n"&gt;sentiment_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;analyzer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;compound&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
   &lt;span class="n"&gt;sentiment_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="c1"&gt;# Create a DataFrame with the dates and sentiment scores&lt;/span&gt;
&lt;span class="n"&gt;sentiment_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sentiment_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_scores&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="c1"&gt;# Calculate average sentiment scores for each date&lt;/span&gt;
&lt;span class="n"&gt;average_sentiment_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sentiment_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sentiment_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# Convert date column to datetime format with specified format&lt;/span&gt;
&lt;span class="n"&gt;average_sentiment_scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_sentiment_scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%m/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/%y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Sort the average sentiment scores DataFrame by date in ascending order&lt;/span&gt;
&lt;span class="n"&gt;average_sentiment_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_sentiment_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In general, the results showed that most of the positive event articles exhibited positive average sentiment scores, confirming that the event was indeed reflected in the news articles with positive sentiment. However, we acknowledged that the dataset was very limited in size, which was going to make it challenging to obtain reliable results in our regression analysis in the future. Consequently, instead of calculating the average for each date, we decided to calculate the individual sentiment scores from all the articles we had scraped, which provided us with a larger dataset to conduct our regression analysis. Below, we show the initial outcome of the average sentiment scores on the left-hand side, which consists of 16 datasets, and the updated sentiment score calculation on the right-hand side, which consists of 85 datasets. Another important point to mention is that in our first attempt, the date of the first news article was mistakenly recorded as July 6th instead of the correct date, which is July 9th. This happened because when we scraped the articles, some of them failed to process the dates and showed NaN, and we had to manually input the dates ourselves. We discovered this error and successfully incorporated the corrected date in our final version of the sentiment analysis. Then, we calculated the sentiment scores for the negative articles by applying the same method. Overall, the sentiment analysis process provided us with valuable lessons, such as the significance of dataset size and data quality control.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing average sentiment scores on the left-hand side, which consists of 16 datasets" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_02_lefthand-sentiment-score.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing updated sentiment score calculation on the right-hand side, which consists of 85 datasets" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_02_righthand-sentiment-score.jpg"&gt;&lt;/p&gt;
&lt;h2&gt;Stock Volatility:&lt;/h2&gt;
&lt;p&gt;While working on our project, we struggled to determine the best method to calculate and analyze the volatility in the best manner. In the beginning, our team considered using the traditional method of computing the annualized standard deviation of daily returns, which was the conventional approach to calculating volatility. The method was quite simple, but our existing dataset has data that is of a narrow time horizon. After reviewing extensive literature, we concluded that this method is used most of the time to study longer and fixed-time volatility conditions and may not be responsive to short-term sharp fluctuations. In our case, it may not be useful because our project is to capture rapid changes in volatility within a month of a news release.&lt;/p&gt;
&lt;p&gt;To address the problem, we turn to the rolling volatility method. This method calculates the standard deviation of returns within a moving window, which is more sensitive to changes, suitable for short-term risk, and more in line with our project objectives. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yfinance&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;yf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;SBUX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2018-04-01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2018-05-01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Returns&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Adj Close&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pct_change&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rolling_Volatility&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Returns&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rolling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;252&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;start_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2018-04-16&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2018-04-27&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start_date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Rolling_Volatility&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rolling_volatility&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Calculated Rolling Volatility:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;SBUX_Rolling_Volatility_April_2018.csv&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;read_back_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Data read from CSV:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;read_back_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The Python code above showed how we calculated volatility when negative news occurred. We obtained historical stock data of Starbucks (ticker: SBUX) for the month of April 2018 using the “yfinance” library. From this data, daily returns were calculated based on the adjusted closing price of the stock. Then, using the returns, we computed rolling volatility for the 2-day window and then annualized it by multiplying it with the square root of 252 (the normal trading days in a year). We then screened the data by specifically filtering for the date range of April 16, 2018, to April 27, 2018, as this was the actual range of dates when the negative news had occurred. The column names of the generated data frame were then converted into lowercase for consistency. We displayed the rolling volatility of the returned results with a date range saved to the CSV file, “SBUX_Rolling_Volatility_April_2018.csv,” using the “date” column as an index. Lastly, this code read the saved CSV file back into a new data frame. Finally, it read the saved CSV file back into a new data frame to ensure that the data was stored properly and in the right format. Finally, it printed the data to display what it contained.&lt;/p&gt;
&lt;p&gt;When computing rolling volatility, we broadened the data range to include more than just the specific period being analyzed. For example, we might collect a full month's worth of stock data, even if our focus is on a shorter timeframe. This approach helps address edge effects by ensuring that calculations at the boundaries of our dataset are not skewed due to insufficient data. This ensures better consistency and accuracy throughout the analysis period and is also a step towards statistical robustness. The "warm-up" period in our model ensures that from the start, the rolling window benefits from a sufficient quantity of data, thus mitigating any potential inaccuracies caused by a limited number of data points. By incorporating a broader dataset, this approach enhances the model's ability to estimate volatility more accurately and stabilizes the impact of market variability. &lt;/p&gt;</content><category term="Progress Report"></category><category term="Group ESG &amp; Volatility"></category></entry><entry><title>Limitations of traditional NLP, Rise of GPT-based algorithms (Group "NewPotential")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/limitations-of-traditional-nlp-rise-of-gpt-based-algorithms-group-newpotential.html" rel="alternate"></link><published>2024-04-29T16:30:00+08:00</published><updated>2024-04-29T16:30:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-29:/FINA4350-student-blog-2024-01/limitations-of-traditional-nlp-rise-of-gpt-based-algorithms-group-newpotential.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To recapitulate our project, we delve into analyzing the sentiment of gold. Through trial and error, we initially set our goal on analyzing sentiments around various assets. Our first trial involved developing an NLP model to categorize news articles about "Tesla" as either bullish or bearish. However, we noticed …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To recapitulate our project, we delve into analyzing the sentiment of gold. Through trial and error, we initially set our goal on analyzing sentiments around various assets. Our first trial involved developing an NLP model to categorize news articles about "Tesla" as either bullish or bearish. However, we noticed that our model wasn't just picking up articles directly related to "Tesla" the company; it was also gathering content that merely mentioned "Tesla" in passing, leading to collecting some irrelevant data. &lt;/p&gt;
&lt;p&gt;This challenge prompted us to pivot our focus. Precious metals, and gold in particular, stood out as an appropriate topic to analyze for our sentiment analysis. Unlike other stocks or bonds, which are often mentioned in various non-relevant contexts, gold presented a more straightforward subject matter for sentiment analysis. Furthermore, our decision was bolstered by the current economic climate. In times of heightened inflation and uncertainty, gold traditionally emerges as a safe asset, so-called “safe-haven.” People's inclination towards gold in these conditions made it an even more compelling subject for our analysis. &lt;/p&gt;
&lt;p&gt;One of the most notable challenges we encountered was the initial difficulty in scraping the most recently published articles. We discovered that the layout and format of articles could vary slightly, even within the same CNBC publication. For instance, a standard CNBC article might be structured quite differently from a CNBC article that has been selected in the web scraping process. This variability initially hindered our ability to consistently scrape the latest news. In response, we refined our code to accommodate these variations, enhancing our model to fetch timely information. This adjustment was crucial in ensuring that the articles we scraped remain up to date, so that they can capture the most relevant sentiment around gold. &lt;/p&gt;
&lt;p&gt;During the web scraping process in our project, the team relied on searching CNBC by the keyword “gold” in hopes of finding the sentiment of gold through news articles. However, it came to our attention that irrelevant news articles were also included, as titles of those articles had the word “gold.” Instead of talking about the commodity, they were often about credit cards or company news with “gold” in their names. This blurred the focus of our sentiment analysis as not much could be done to deal with this issue. &lt;/p&gt;
&lt;p&gt;Our decision to focus on gold, rather than other assets like "Apple" stocks, was not made lightly. Beyond the practical considerations around data relevance and clarity, gold's unique position in the financial market offered rich insights to mine. Unlike other assets, gold tends to retain value or even appreciate even in the fluctuating market, making it a unique asset class. Its role as a hedge asset against inflation and economic uncertainty makes the sentiment around gold a particularly intriguing subject to choose.  &lt;/p&gt;
&lt;h2&gt;Lack granularity required to study a particular subject&lt;/h2&gt;
&lt;p&gt;It was observed that the sentiment scores obtained were distracted by different noises. The mechanism behind NLP is simple as it gives out predefined score to words in a sentence according to information from database.  If the piece of news article was comparison between two different commodities, it will carry on giving out score based on the context, disregarding the comments on the commodity that was irrelevant to gold and possibly computed a result contrary to this project’s goal. Putting the effort into trending asset classes, such as stock of Apple, this limitation may have more significant hindering effect as it is often evaluated alongside big tech companies like Tesla. To remove such noises, Named Entity Recognition (NER) may help to some extent, as a technique within NLP that identifies and categorizes named entities present in text, such as company names, financial indicators, or relevant keywords. By implementing NER algorithms tailored to financial news, it becomes possible to extract specific entities and their associated sentiments more accurately.  &lt;/p&gt;
&lt;p&gt;Another similar cause to weak and ineffective analysis in the project is the incapability of probing from perspective of finance. The financial domain encompasses a wide range of industry-specific jargon, acronyms, and terminology. These special terms may convey different meanings compared to daily and conventional use. Hence, relying solely on pre-existing and generic NLP packages for understanding these terms may lead to inaccuracies or incomplete comprehension. Therefore, a question has risen. How do we let the machine think in the perspective of finance as humanly as possible? Proper data processing will be key to ensure qualitative input is thrown inside NLP packages. Addressing this with Bag-of-Words (BoW), text is represented as a collection of individual words, disregarding grammar and word order. By creating a thematic corpus of financial terms and phrases, and mapping them to corresponding concepts, a custom BoW model can be developed. A tailored BoW model shall enhance the understanding of financial language, enabling more precise analysis and interpretation of financial news.  &lt;/p&gt;
&lt;p&gt;To touch up the outcome, appropriate NLP packages shall be carefully picked and implemented, while one may also consider available packages particularly for certain subjects. For example, &lt;code&gt;NLTK Vader&lt;/code&gt; performs well in understanding social media context and &lt;code&gt;afinn&lt;/code&gt; is strong in blogs-like platform like Twitter. With much work and fine-tuning to be done, there will be lots of obstacles for first timers to overcome while conduting a proper text analytics, without fearing to lose the analyzing target midway.  &lt;/p&gt;
&lt;h2&gt;The Challenge of Sarcasm and Irony&lt;/h2&gt;
&lt;p&gt;The use of sarcasm and irony can be particularly problematic for NLP models when analyzing market news. These rhetorical devices often convey the opposite of what is literally stated, making it extremely challenging for algorithms to accurately interpret the intended sentiment. &lt;/p&gt;
&lt;p&gt;Consider a headline that reads: "Investors Rejoice as Housing Prices Soar to Unaffordable Levels." On the surface, the use of the word "Rejoice" suggests a positive sentiment, implying that investors are happy about the rising housing prices. However, the underlying sarcasm becomes evident when we consider the second part of the headline, which states that the prices have reached "unaffordable levels." The juxtaposition of "Rejoice" and "unaffordable levels" is a clear indication of sarcasm, where the author is actually expressing a negative sentiment about the housing market situation, despite the seemingly positive wording. &lt;/p&gt;
&lt;p&gt;An NLP model that solely relies on the literal interpretation of the words may incorrectly categorize this headline as conveying a positive sentiment. It would fail to recognize the sarcastic undertone and the true negative sentiment expressed by the author. &lt;/p&gt;
&lt;p&gt;The challenge lies in the fact that sarcasm and irony often rely on nuanced linguistic cues, such as tone, context, and implicit meaning, which can be difficult for algorithms to detect and interpret accurately. Unlike clear-cut emotional expressions, sarcasm and irony are highly contextual and may not follow a predictable set of rules, making it particularly challenging for NLP models to handle.&lt;/p&gt;
&lt;h2&gt;Solution: Developing Specialized Sarcasm Detection Models&lt;/h2&gt;
&lt;p&gt;Researchers have explored the use of machine learning techniques, such as deep learning, to build models specifically trained to detect sarcasm and irony in text. These models can learn to identify linguistic patterns, tone, and other cues that indicate the use of sarcasm. &lt;/p&gt;
&lt;p&gt;The key idea behind this approach is to develop machine learning models that are specifically trained to identify linguistic patterns, tonal cues, and other indicators of sarcastic or ironic expressions in text. &lt;/p&gt;
&lt;p&gt;Here's how this solution can be implemented: &lt;/p&gt;
&lt;p&gt;First, you would need to curate a dataset of market news articles and headlines, where human annotators have manually labeled instances of sarcasm and irony. This annotated corpus would serve as the training data for the sarcasm detection model. &lt;/p&gt;
&lt;p&gt;Next, you would utilize advanced natural language processing techniques, such as deep learning, to build a model that can learn to recognize the distinctive features of sarcastic and ironic language. This could involve training the model to detect subtle linguistic cues, understand contextual information, and identify patterns that signal the use of sarcasm or irony. &lt;/p&gt;
&lt;p&gt;Once the sarcasm detection model is trained, it would be integrated as a separate component within the overall market sentiment analysis pipeline. When analyzing new market news, this specialized model would be used to identify and flag any instances of sarcastic or ironic language. &lt;/p&gt;
&lt;p&gt;Finally, the output from the sarcasm detection model would be leveraged to enhance the sentiment analysis process. For example, if the model identifies sarcasm in a headline, the sentiment score could be adjusted to reflect the underlying negative sentiment, rather than the literal positive wording. &lt;/p&gt;
&lt;p&gt;By implementing this specialized sarcasm detection solution, we can significantly improve the accuracy and reliability of your market sentiment analysis, helping us avoid the pitfalls of misinterpreting the true sentiment expressed in market news.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group NewPotential"></category></entry><entry><title>Dependent Data Selection (by Group "SalesEQ")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/dependent-data-selection-by-group-saleseq.html" rel="alternate"></link><published>2024-04-23T22:00:00+08:00</published><updated>2024-04-23T22:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-23:/FINA4350-student-blog-2024-01/dependent-data-selection-by-group-saleseq.html</id><summary type="html">&lt;h3&gt;Main Contributer for this blog: Maximilian Droschl&lt;/h3&gt;
&lt;h2&gt;Reason for us to switch dependent variable&lt;/h2&gt;
&lt;p&gt;After evaluating several online sources for extracting monthly telephone sales data, including Capital IQ, Bloomberg and telephone company websites, our team realized that a standardized way of extracting the data was unrealistic. In order not to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Main Contributer for this blog: Maximilian Droschl&lt;/h3&gt;
&lt;h2&gt;Reason for us to switch dependent variable&lt;/h2&gt;
&lt;p&gt;After evaluating several online sources for extracting monthly telephone sales data, including Capital IQ, Bloomberg and telephone company websites, our team realized that a standardized way of extracting the data was unrealistic. In order not to risk using poor quality data as the basis for our time series model to make the final prediction, the team concluded that it would be best to change the dependent variable.&lt;/p&gt;
&lt;h2&gt;New dependent - Stock Price&lt;/h2&gt;
&lt;p&gt;To keep the focus on the American phone market, our new dependent variable will be the monthly return of a stock portfolio consisting of the five largest phone companies according to their market share within the American telecommunications market. According to Bloomberg, the companies with the largest market shares in the U.S. smartphone industry are Apple (68.23%), Samsung (25.20%), Motorola (3.90%), and Google (2.67%).&lt;/p&gt;
&lt;p&gt;But why focus on stocks rather than phone sales? Financial markets, particularly stock markets, offer valuable insights into the performance of the smartphone industry. They are considered the most efficient means of reflecting all available information, regardless of its form of efficiency (Fama, 1970). In contrast to other performance measures, such as sales figures, profits, or expenses, financial markets provide a more comprehensive view. Accurately forecasting industry performance or the performance of a particular company within the industry can inform strategies for one's own business. Additionally, since companies have access to their own sales numbers, forecasted asset returns can serve as an informative explanatory variable in models aimed at forecasting sales.&lt;/p&gt;
&lt;p&gt;However, Roberts (1967) demonstrated that there are various forms of efficiency that may lead to a pure approximation of future prices. This is because fundamental analysis relies on publicly available information and cannot generate excess returns. As the literature reports semi-strong form efficiency and occasional violations of it, we aim to provide more accurate forecasts by adding sentiment scores derived from our Sentiment-LDA model, in addition to publicly available information.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, the final time series model (ARIMAX/LSTM) will be fitted to the subsequent dataset: Dependent Variable - Monthly returns of equity portfolio; Explanatory Variables - Lagged returns of equity portfolio, Moving Average term, Residual term, Exogenous variables, which comprise the series of sentiment scores from our Sentiment-LDA model and the Michigan Consumer Sentiment Index.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Fama, E. F. (1970). Efficient capital markets. Journal of finance, 25(2), 383-417.&lt;/p&gt;
&lt;p&gt;Roberts, H. (1967). Statistical versus clinical prediction of the stock market CRSP. University of Chicago, Chicago.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group SalesEQ"></category></entry><entry><title>Data Cleaning and Analysis (by Group "SalesEQ")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/data-cleaning-and-analysis-by-group-saleseq.html" rel="alternate"></link><published>2024-04-21T16:00:00+08:00</published><updated>2024-04-21T16:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-21:/FINA4350-student-blog-2024-01/data-cleaning-and-analysis-by-group-saleseq.html</id><summary type="html">&lt;h3&gt;Main Contributer for this blog: Ricky Choi (Data Cleaning), Zixian Fan (Data Analysis)&lt;/h3&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Data Cleaning is an important part in text preprocessing as it determines the quality of data and hence the performance of the model. The goal of text preprocessing is to remove redundant text that does …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Main Contributer for this blog: Ricky Choi (Data Cleaning), Zixian Fan (Data Analysis)&lt;/h3&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Data Cleaning is an important part in text preprocessing as it determines the quality of data and hence the performance of the model. The goal of text preprocessing is to remove redundant text that does not help with explaining the sentiment of the articles, and to unify the formats of the texts. &lt;/p&gt;
&lt;p&gt;The first step of cleaning is to check for presence for abnormal data in the scraped csv file. From my personal experience, abnormal data may be some meaningless data with very little word count. Therefore, we checked for any news articles with word count below 20 words, and fortunately no abnormal data was discovered in the csv file. &lt;/p&gt;
&lt;p&gt;Next, we converted all texts to lowercases, followed by removal of redundant data such as stop words, special characters, numbers, and synonyms. Then we performed lemmatization to reduce words into their root form. Details will be covered in the report.&lt;/p&gt;
&lt;p&gt;There are a few limitations we encountered during data cleaning. First, spelling mistakes cannot be eliminated. There are no patterns for spelling mistakes, and it is not impossible to do eyeball check on all words and correct them. Given the sources of text are mainly reputable newspapers such as the New York Times and the Washington Post, error due to spelling mistakes can be neglected. Second, the original meaning of some words may be lost after transformation. For instance, “U.S.” becomes “us” after lowercasing and removal of punctuation. It may be misrecognized as the pronoun “us” instead of a country. We believe this problem does not occur frequently and hence it can be neglected. &lt;/p&gt;
&lt;h2&gt;Data Analysis&lt;/h2&gt;
&lt;p&gt;After obtaining the cleaned news data, we started to perform some necessary analyses on the data in order to better go about applying subsequent machine learning models. We divided it into two parts: rudimentary analysis and text analysis.&lt;/p&gt;
&lt;h3&gt;Rudimentary analysis&lt;/h3&gt;
&lt;p&gt;Firstly, we have the general understanding of the dataset with:&lt;/p&gt;
&lt;p&gt;Total line of text: 12568.&lt;/p&gt;
&lt;p&gt;Monthly period number: 132.&lt;/p&gt;
&lt;p&gt;And, we can have a quick look of our dataframe:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-1.png"&gt;&lt;/p&gt;
&lt;p&gt;From the figure, we can see that the data frame mainly consists of these five columns, where Cycle Month indicates which month of the year the article belongs to, because the model we are going to apply subsequently is learning for monthly data, which is equivalent to adjusting the frequency of the data. Headline is the headline of these news, and Text represents the news contents.&lt;/p&gt;
&lt;p&gt;After that, we went to analyze how much news we were able to retain each month, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Since we are capped at 100 news articles per acquisition, that number of news articles will be available in the beginning years. However, the number of relevant news obtained has declined in recent times, probably because the market's attention was not primarily on mobile phones during this period. However, since the number of news articles is roughly higher than 50, it does not unduly affect our final results.&lt;/p&gt;
&lt;p&gt;In the meantime, we briefly analyzed the source of the text data and found that it mainly came from Reuters News and Wall Street Journal. Perhaps in the future, those who follow mobile news could check these two data sources more often.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-3.jpg"&gt;&lt;/p&gt;
&lt;h3&gt;Text Analysis&lt;/h3&gt;
&lt;p&gt;Afterwards we would like to perform some simple analyses of the obtained text in order to better understand the data. We are going to do the analysis based on the following two pieces of code:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-4.png" alt="Input Code" width="400" height="100"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-5.png" alt="Input Code" width="400" height="100"&gt;&lt;/p&gt;
&lt;p&gt;These codes get the five most frequent words for each month. Inevitably, however, the main words are a lot of useless connectives and articles&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-6.png"&gt;&lt;/p&gt;
&lt;p&gt;So we introduce nltk's stopwords to sift out these useless words. Stopwords are words similar to: {'a','about','above','after','again','against','ain','all','am','an','and','any','are','aren',"aren't",'as','at','be','because','been','before','being','below','between','both'} &lt;/p&gt;
&lt;p&gt;We further merge the data as well as make statistics to get the following dataframe. For example, in January 2012, the most important words in the headline were update, apple, loss, profit and sales, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-7.png" alt="Input Code" width="400" height="350"&gt;&lt;/p&gt;
&lt;p&gt;Finally, we construct Word Cloud and Word Co-occurrence Network Graphs to make some more intuitive interpretations. Word Cloud shows which words appear most frequently in all text, and the higher the frequency the larger the word. We can see that words like Microsoft, profit, said, Lenovo are the most important words in mobile phone related news.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-8.jpg" alt="Input Code" width="500" height="450"&gt;&lt;/p&gt;
&lt;p&gt;The Word Co-occurrence Network Graphs shows the co-occurrence relationships between words in text data. In this graph, each node represents a word, and the edges indicate how often these words occur in the same text. The positions of the nodes are determined based on a network analysis algorithm, and their positions usually reflect the relationships between the nodes. Nodes that are closer together indicate that they co-occur more frequently in the text. And the size of the nodes can be adjusted according to the frequency of occurrence of the words, and words that occur more frequently are usually shown as larger nodes. The number of edges indicates the number of occurrences between two words, and the thickness of edges is usually proportional to the frequency of occurrences.
From there, we can see in the chart below that words like company, apple, china, and samsung are the most frequent words that appear alongside other words. The occurrence of pandemic, trump, and russia at the edges also gives us a sideways view of the significant times in which these words appear.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_03_img-9.jpg"&gt;&lt;/p&gt;
&lt;h2&gt;Final Remark&lt;/h2&gt;
&lt;p&gt;From the Data Cleaning and Data Analysis sections above, we gained a better understanding of our textual data, which prepared us for the subsequent work of understanding text and thinking about how to transform it to ask quantifiable variables.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group SalesEQ"></category></entry><entry><title>Data Web Scraping (by Group "SalesEQ")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/data-web-scraping-by-group-saleseq.html" rel="alternate"></link><published>2024-04-17T22:00:00+08:00</published><updated>2024-04-17T22:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-04-17:/FINA4350-student-blog-2024-01/data-web-scraping-by-group-saleseq.html</id><summary type="html">&lt;h3&gt;Main Contributer for this blog: Jason Li&lt;/h3&gt;
&lt;h2&gt;Web Scraping Review&lt;/h2&gt;
&lt;p&gt;Based on a little web scraping experience in the past in other courses and projects, we believe web scraping is a bit like a data exploration task, which is to locate patterns in the tags of the information we need …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Main Contributer for this blog: Jason Li&lt;/h3&gt;
&lt;h2&gt;Web Scraping Review&lt;/h2&gt;
&lt;p&gt;Based on a little web scraping experience in the past in other courses and projects, we believe web scraping is a bit like a data exploration task, which is to locate patterns in the tags of the information we need and try to put everything into a loop. For this time, we still need to locate such patterns, with a lot more other factors/issues to be considered.&lt;/p&gt;
&lt;p&gt;Compared to our previous experience in web scraping, web scraping on Factiva is a much less static task. For starters, we can not directly access the Factiva platform with a hyperlink, otherwise we would be redirected to a login page which we do not have the credentials to login. Thus, we have to start from the HKU library page step-by-step, login through the library page with our HKU library credentials to finally enter the familiar query page.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-2.png" alt="Input Code" width="300" height="200"&gt;&lt;/p&gt;
&lt;h2&gt;Enter the Page&lt;/h2&gt;
&lt;p&gt;Upon entering this page, there are plenty of options to be toggled or adjusted to query the relevant articles that we would like, which took us quite a while as there are various types of actions to be done:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-3.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;⁠Free Text Input&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-4.png"&gt;
&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-5.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Dropdown menu selection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-6.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-7.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search and select&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-8.png"&gt;&lt;/p&gt;
&lt;h2&gt;Small Bugs to pay attention to&lt;/h2&gt;
&lt;h3&gt;1&lt;/h3&gt;
&lt;p&gt;There are plenty of hiccups throughout the process of interacting with this query page. Sometimes the pages took too long to load, causing timeout exceptions and sometimes we accidentally intervened with my mouse and the search bar does not return an option for us to select. On very rare occasions, the page is not properly loaded and is displayed in a html format or the html tags mysteriously change slightly to throw us a curveball. Considering we would like to scrape articles from each month across 10 years, this process has to be repeated 120 times and the myriad of errors has been rather frustrating. However, this is part of web scraping, trial-and-error to ensure the robustness of the code.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-9.png"&gt;&lt;/p&gt;
&lt;h3&gt;2&lt;/h3&gt;
&lt;p&gt;The search result page is rather standard with a clear ‘row-by-row’ structure. Considering the limitations of the HKU license, it would be preferred to not use automation software to download these articles directly. To work around this, the program clicks into these articles one by one and scrapes the headlines and the bodies of these articles, then goes back to the previous search result page. However, these articles come in different structures and formats, with different numbers of paragraphs, and sometimes with photos or tables in between. Unfortunately, we have attempted different methods to no avail, which can only leave the task of cleaning these texts to the preprocessing afterwards.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-10.png"&gt;&lt;/p&gt;
&lt;h3&gt;3&lt;/h3&gt;
&lt;p&gt;Looping through these articles presents some issues as well. For instance, the server would return a gateway error, which is rare but rather unfixable. On the other hand, it is quite common that during the process of going back to the search result page, the page would somehow go all the way back to the query page. The tricky part is that the query is partially deleted i.e. data sources reset and the html tags would differ slightly, which takes extra effort to handle and not disrupt the scraping. Eventually, the run took about 8-9 hours to complete.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-11.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Input Code" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/SalesEQ_02_img-12.png"&gt;&lt;/p&gt;
&lt;h2&gt;Final remark from the main contributor of this part&lt;/h2&gt;
&lt;p&gt;After completing the task, we find web scraping a rather unglamorous, slightly frustrating but rewarding experience, more so compared to my previous encounters with web scraping. In reports or presentations, data collection is often briefly skimmed through, but is certainly a crucial part of any project.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group SalesEQ"></category></entry><entry><title>Blog Post One - Initial Thoughts (by Group "Textonomy")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/blog-post-one-initial-thoughts-by-group-textonomy.html" rel="alternate"></link><published>2024-03-04T22:30:00+08:00</published><updated>2024-03-04T22:30:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/blog-post-one-initial-thoughts-by-group-textonomy.html</id><summary type="html">&lt;h2&gt;The Process We Come Up With Our Topic&lt;/h2&gt;
&lt;p&gt;Without a doubt, we are now in an Artificial Intelligence (AI) era. Within a few years, we went from traditional Deep Learning models to seemingly close to being near Artificial General Intelligence, and OpenAI just released Sora which amazed the world. With …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;The Process We Come Up With Our Topic&lt;/h2&gt;
&lt;p&gt;Without a doubt, we are now in an Artificial Intelligence (AI) era. Within a few years, we went from traditional Deep Learning models to seemingly close to being near Artificial General Intelligence, and OpenAI just released Sora which amazed the world. With the increasing popularity of AI, the demand for AI-related products has also increased, including Nvidia’s high-performance advanced graphics processing units (GPUs). The advanced chips have become a critical component in AI development, powering deep learning algorithms and accelerating computational tasks in different fields, ranging from healthcare and finance to manufacturing and transportation. As a result, Nvidia’s stock reached its historical peak of USD 823.94 on 23-02-2024 from USD 492.44 on 02-01-2024, a substantial increase of 67.32%, making it an attractive investment option for those seeking exposure to the flourishing AI sector.&lt;/p&gt;
&lt;p&gt;The recent phenomenal surge of demand in overall AI technology and Nvidia's stock price rise have inspired our group to apply text analytics and Natural Processing Language (NLP) techniques to analyze investors’ sentiment toward AI development and its influences on AI sector stocks.&lt;/p&gt;
&lt;p&gt;Our group believes the analysis of investors' sentiment toward AI development is important for understanding market dynamics and identifying trends that can potentially impact asset allocations and trading decisions. By leveraging text analytics techniques, such as sentiment analysis and opinion mining, our group aims to extract valuable financial insights from textual data sources, including news articles and social media posts, to analyze how positive or negative sentiment towards AI development and consider it as a systematic risk of AI sector stocks.&lt;/p&gt;
&lt;h2&gt;The Potential Ways to Achieve Our Goals&lt;/h2&gt;
&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;For our sentiment analysis project focusing on AI discourse, we aim to collect data from sources talking about financial information regarding AI-related companies such as news websites like &lt;a href="https://finance.yahoo.com/"&gt;Yahoo Finance&lt;/a&gt;, which provides extensive financial news coverage, including AI developments, or the Financial Times. To diversify our dataset, we consider platforms like &lt;a href="https://www.reddit.com/"&gt;Reddit&lt;/a&gt;, where community discussions can offer valuable insights into public sentiment.&lt;/p&gt;
&lt;p&gt;Our web scraping strategy involves tools such as &lt;a href="https://www.crummy.com/software/BeautifulSoup/"&gt;Beautiful Soup&lt;/a&gt; for parsing HTML/XML documents, Requests for HTTP requests, and &lt;a href="https://www.selenium.dev/"&gt;Selenium&lt;/a&gt; for dynamic content. Compliance with each platform's guidelines ensures ethical data collection. We'll organize the scraped data, including article headlines, publication dates, and texts, for preprocessing.&lt;/p&gt;
&lt;p&gt;The preprocessing phase is crucial for cleaning and standardizing the data. We plan to remove HTML tags, special characters, and whitespace, normalize text to lowercase, and apply tokenization, stop words removal, and lemmatization using libraries like &lt;a href="https://www.nltk.org/"&gt;NLTK&lt;/a&gt; and &lt;a href="https://spacy.io/"&gt;spaCy&lt;/a&gt;. These steps are designed to refine the dataset, making it suitable for sentiment analysis through iterative scripting and quality checks.&lt;/p&gt;
&lt;p&gt;For pretraining data, we have searched online and found some potentially useful labeled datasets, such as &lt;a href="https://github.com/zfz/twitter_corpus"&gt;Sanders&lt;/a&gt; (Twitter sentiment corpus), &lt;a href="https://ieee-dataport.org/open-access/stock-market-tweets-data"&gt;Taborda&lt;/a&gt; (stock market tweets data), &lt;a href="https://huggingface.co/datasets/financial_phrasebank"&gt;financial phrase-bank&lt;/a&gt; (financial news headings and sentiments), and &lt;a href="https://sites.google.com/view/fiqa/"&gt;FiQA&lt;/a&gt; (financial news, microblog messages, and their sentiments). They may have different data structures and label methods, and we will preprocess them to form a large uniform dataset for training, validation, and testing.&lt;/p&gt;
&lt;h3&gt;Model Selection &amp;amp; Training&lt;/h3&gt;
&lt;p&gt;Considering the limited time and computing resources, we plan to select a middle-size language model such as Bert, or FinBert (Bert with some financial knowledge pretraining). The goal is to train the model to take in texts and produce a sentiment score (e.g., 1 for very negative and 10 for very positive) with our collected pretraining datasets. Another potential way to train the model is to make use of large language models like ChatGPT and Bard. We can use the large ones as teachers producing fake training data and teach our middle-size model how to predict. In this way, we expect to get a model performing slightly worse than the large ones or on par with them. In the last stage, we will apply the trained Bert model to our collected real-life data from the Internet.&lt;/p&gt;
&lt;h3&gt;Potential Uses of Results&lt;/h3&gt;
&lt;p&gt;With the sentiment scores, we can get a general idea of the market sentiment toward AI development. For example, we may use the weighted average score of all texts analyzed as the overall market sentiment. We are then able to predict whether the opening price of AI sector stocks will rise or fall on the next day based on the overall sentiment on that day. In addition, we intend to find the correlation between the sentiment and the price change so that we can even predict how much will the price rise or fall. If time permits, we will also design trading strategies based on the results of the model predictions.&lt;/p&gt;
&lt;h2&gt;Expected Challenges During The Project&lt;/h2&gt;
&lt;p&gt;As now the project is in its early stages, we expect to encounter some challenges in detailed tasks. Firstly, the data sources we selected may have an anti-crawler mechanism (our project is for academic purposes only), which limits our ability to obtain large datasets. Secondly, the collected data may not be suitable for model training or prediction. Thirdly, how we weigh the scores across all texts needs to be discussed. Lastly, the correlation between the market sentiment toward AI development and the price of AI sector stocks may not be significant as there are also other factors influencing the market.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group Textonomy"></category></entry><entry><title>Project Introduction – Predicting Smartphone Sales in the United States (by Group "SalesEQ")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/project-introduction-predicting-smartphone-sales-in-the-united-states-by-group-saleseq.html" rel="alternate"></link><published>2024-03-04T18:00:00+08:00</published><updated>2024-03-04T18:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/project-introduction-predicting-smartphone-sales-in-the-united-states-by-group-saleseq.html</id><summary type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;p&gt;Zixian Fan, majoring in Statistics, with a second major in Finance, and a minor in Computer Science. Loves math, loves quantitative research, and has a sweet tooth. Loves to play games, with previously being in the Legends segment of Hearthstone.
&lt;a href="https://github.com/FanZixian"&gt;https://github.com/FanZixian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maximilian Droschl, Bachelor in …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;p&gt;Zixian Fan, majoring in Statistics, with a second major in Finance, and a minor in Computer Science. Loves math, loves quantitative research, and has a sweet tooth. Loves to play games, with previously being in the Legends segment of Hearthstone.
&lt;a href="https://github.com/FanZixian"&gt;https://github.com/FanZixian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Maximilian Droschl, Bachelor in Economics with focus on Econometrics and Data Science, Exchange Student from the University of St. Gallen. Has a passion for chess and loves playing lacrosse. 
&lt;a href="https://github.com/MDrschl"&gt;https://github.com/MDrschl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ricky Choi, double degree in Computer Science and Finance at HKU. Loves teamsports, working out, and playing snooker. Also enjoys watching movies and animes. 
&lt;a href="https://github.com/Rickycmk"&gt;https://github.com/Rickycmk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jason Li, majoring in Quantitative Finance and minoring in Computer Science. Loves playing sports like volleyball and football. Also a lover of sitcoms and movies.
&lt;a href="https://github.com/Jasonlcyy"&gt;https://github.com/Jasonlcyy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mahir Faiaz, Bachelor of Arts and Sciences in Financial Technology. An avid international debater and a silly soccer lover.
&lt;a href="https://github.com/MahirFaiaz"&gt;https://github.com/MahirFaiaz&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Objective of this blog&lt;/h2&gt;
&lt;p&gt;This blogpost is intended to provide a surface-level idea about the ins and outs of our project. We will elaborate on the exact objective of our project, and then illustrate possible mechanisms to reach our end goals. As we do that, we will also be discussing the viability of our proposed methods and our targeted data sources.&lt;/p&gt;
&lt;p&gt;This blogpost is designed to allow the reader to get a good insight into the types of discussions we indulged in to get to this stage of the project. We believe such a practice will allow the reader to get a deeper understanding of our motivations and the ability to grasp what events led to a given outcome. In the long run, this should give the Professor a proper timeline of our project and an idea about the group atmosphere at any given point in the timeline, leading to the possibility of a more comprehensive evaluation.&lt;/p&gt;
&lt;h2&gt;Outline of the Project&lt;/h2&gt;
&lt;p&gt;The overarching objective of our project will be to predict smartphone sales in the United States. This is primarily motivated by the fact that the smartphone industry has become increasingly competitive in recent years, forcing manufacturers to develop new strategies. One of these strategies is forecasting industry sales, which, if mismanaged, can have significant consequences. The rapid pace of product development, increasing differentiation among smartphones, and relatively short life cycles of smartphones contribute to unpredictable sales patterns and increased volatility, exacerbating the challenge. Traditional models are mostly based on past values of the sales series itself, variables related to the product, such as its price or the brand, consumer sentiment indices, and economic variables such as the consumer price index or stock indices. However, we are motivated to extend these traditional techniques to a hybrid forecasting model that aims to incorporate sentiment indices estimated on the basis of text data related to phone sales. Thereby, we test the hypothesis whether sentiment scores derived from news articles add predictive information to traditional model specifications.&lt;/p&gt;
&lt;h2&gt;Modeling and data scraping&lt;/h2&gt;
&lt;p&gt;In general, our approach employs three different models. An LDA model is used to properly extract information from large amounts of text data with content related to smartphone sales in the United States. The final prediction is then made by an ARIMAX model, which has the advantage of being able to interpret the significance of each predictor variable included in the model, while at the same time allowing the modeling of non-stationary multivariate data. For comparison reasons, an LSTM model will be fitted to the data.&lt;/p&gt;
&lt;p&gt;Note that we haven’t decided which text data to be included in our research, because it is not determined what websites, articles, forums are available for web scraping or whether these data are easy to process and effective to our model. Even after narrowing down the scope, the reliability and relevance of these sources are not necessarily ensured. The source and the effective method to query reliable and relevant articles/tweets are problems that need to be sorted. Although, we would like to mention some text data candidates that might be applied in our text analysis:&lt;/p&gt;
&lt;p&gt;Seeking Alpha
Twitters
Some articles from free financial journals&lt;/p&gt;
&lt;p&gt;Additionally, as the data is not determined yet, we cannot give the name of our exogenous variables that will be applied to the ARIMAX/LSTM model. Hence, they are also not included in this blog post, but will be included in our following blogs soon as soon as we have fitted them to the data.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;Farkhod, A., Abdusalomov, A., Makhmudov, F., &amp;amp; Cho, Y. I. (2021). LDA-Based Topic Modeling Sentiment Analysis Using Topic/Document/Sentence (TDS) Model. Applied Sciences, 11(23), 11091.&lt;/p&gt;
&lt;p&gt;Ali, T., Omar, B., &amp;amp; Soulaimane, K. (2022). Analyzing Tourism Reviews Using an LDA Topic-Based Sentiment Analysis Approach. MethodsX, 101894.&lt;/p&gt;
&lt;p&gt;Sa-Ngasoongsong, A., Bukkapatnam, S. T., Kim, J., Iyer, P. S., &amp;amp; Suresh, R. P. (2012). Multi-step sales forecasting in automotive industry based on structural relationship identification. International Journal of Production Economics, 140(2), 875-887.&lt;/p&gt;
&lt;p&gt;Hwang, S., Yoon, G., Baek, E., &amp;amp; Jeon, B. K. (2023). A Sales Forecasting Model for New-Released and Short-Term Product: A Case Study of Mobile Phones. Electronics, 12(15), 3256.&lt;/p&gt;
&lt;p&gt;Jadhav, T. (2020). Prediction of Cell Phone Sales from Online Reviews Using Text Mining. International Journal of Research in Engineering, Science and Management, 3(8), 214-218.&lt;/p&gt;
&lt;p&gt;Lassen, N. B., Madsen, R., &amp;amp; Vatrapu, R. (2014, September). Predicting iphone sales from iphone tweets. In 2014 IEEE 18th International Enterprise Distributed Object Computing Conference (pp. 81-90). IEEE.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group SalesEQ"></category></entry><entry><title>Setbacks from News Sentiment Analysis (by Group "NewPotential")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/setbacks-from-news-sentiment-analysis-by-group-newpotential.html" rel="alternate"></link><published>2024-03-04T16:59:00+08:00</published><updated>2024-03-04T16:59:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/setbacks-from-news-sentiment-analysis-by-group-newpotential.html</id><summary type="html">&lt;p&gt;Authors: &lt;/p&gt;
&lt;p&gt;Thong Pei Cheng (Ocean) &lt;/p&gt;
&lt;p&gt;Jang Jungmin &lt;/p&gt;
&lt;p&gt;Wong Nicole &lt;/p&gt;
&lt;p&gt;Yao Yi Tung &lt;/p&gt;
&lt;p&gt;Yeung Cheuk Hin &lt;/p&gt;
&lt;h2&gt;Introduction:&lt;/h2&gt;
&lt;p&gt;In today's rapidly evolving financial landscape, the fusion of technology and finance has become increasingly intertwined. As a diverse team with academic backgrounds in fintech and quantitative finance, coupled with varied working experiences ranging …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Authors: &lt;/p&gt;
&lt;p&gt;Thong Pei Cheng (Ocean) &lt;/p&gt;
&lt;p&gt;Jang Jungmin &lt;/p&gt;
&lt;p&gt;Wong Nicole &lt;/p&gt;
&lt;p&gt;Yao Yi Tung &lt;/p&gt;
&lt;p&gt;Yeung Cheuk Hin &lt;/p&gt;
&lt;h2&gt;Introduction:&lt;/h2&gt;
&lt;p&gt;In today's rapidly evolving financial landscape, the fusion of technology and finance has become increasingly intertwined. As a diverse team with academic backgrounds in fintech and quantitative finance, coupled with varied working experiences ranging from tech roles to private banking, we are united by our shared belief in the profound applications of NLP in the financial industry.  &lt;/p&gt;
&lt;p&gt;To kickstart our project, we have chosen to analyse news articles and determine the sentiment surrounding the assets (e.g. stocks, bonds, commodities, etc) mentioned within them. By extracting sentiment from these articles, we aim to identify whether the mentioned assets are likely to experience positive or negative sentiment in the market. This approach will enable us to gain a deeper understanding of how news impacts market dynamics and potentially uncover valuable insights for investors. &lt;/p&gt;
&lt;p&gt;In this blog post, we will document our learning journey, and describe how we have come to a consensus on exploring this topic. we will present our findings thus far, shedding light on the relationship between news sentiment and financial market performance. &lt;/p&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;Before we delve into our present proposal, it's important to look back and reflect on our past ideas that didn't quite make the cut. We initially brainstormed a few concepts, such as developing a chatbot or a ChatGPT tailored to the financial data of companies, predicting loan approval, and analyzing news to generate summaries. In the end, we gravitated towards analyzing news with a focus on sentiment (positive/negative), and whether it was name-based or asset-based. &lt;/p&gt;
&lt;p&gt;Firstly, we entertained the idea of feeding a chatbot or ChatGPT with company annual reports. We envisioned a system where users could ask questions or search for keywords, akin to a Google search, and the chatbot would produce reliable answers. For example, in the realm of financial regulations, users could input legal terms, and the chatbot, powered by Python ChatterBot, would provide specific answers, much like a digital attorney. However, we eventually dropped this idea, taking into account that adopting a more recent and reliable than ChatGPT-2 would incur considerable expense. This led us to brainstorm another idea that could potentially make a more meaningful impact on people’s lives. &lt;/p&gt;
&lt;p&gt;Thus, after our first idea, we came close to initiating a project centered on loan approval prediction. The idea was to forecast the probability of loan approval based on customer data, such as credit score, income, employment type, and the purpose of the loan. This would potentially facilitate a smoother and quicker guidance for customers, sparing them the hassle of visiting banks and enduring lengthy procedures. However, the issue we encountered was, first, the difficulty in sourcing personal information, specifically variables like credit score and income. While we did consider having customers provide their personal information via a Google form or survey, after discussing with our professor, we concluded that we should reconceptualize and aim for a topic that encompasses finance more broadly. Apart from that, loan approval may heavily focus more on numerical data, such as the credit score of an individual or total asset value that an entity holds. Text analytics may play a less important role in decision making, which weakened the significance of final predicted result. &lt;/p&gt;
&lt;p&gt;The closest idea to what we aim to achieve with everyday financial news was summarizing financial news articles on a daily basis driven by our group members' firsthand experience. Most of us had undergone the tedious process of manually summarizing news from 6 P.M. to 6 A.M. for morning updates during our internships.  Consequently, we thought that creating a tool to automate daily news updates could potentially alleviate the workload for interns in similar positions. However, we failed at fulfilling the element of finance in this news aggregator. So, these previous "failures" have informed and shaped our current direction: providing a quicker and better understanding to investors of diverse market trends by analyzing news with three different sentiments. &lt;/p&gt;
&lt;h2&gt;Why this project?&lt;/h2&gt;
&lt;p&gt;The world of finance and investing is highly dynamic, with asset prices influenced by a multitude of factors, including market trends, economic indicators, and company-specific news. Staying informed about these factors is crucial for making informed investment decisions. However, the sheer volume and complexity of news articles and reports can make it challenging for investors to quickly and accurately gauge the sentiment surrounding particular assets. &lt;/p&gt;
&lt;p&gt;To address this challenge, the development of a news sentiment analysis tool specifically tailored to follow the trend of a certain asset offers significant value. Such a tool would leverage advanced natural language processing and machine learning techniques to automatically monitor and analyze news articles, press releases, and other relevant sources of information about the asset. &lt;/p&gt;
&lt;p&gt;The primary motivation for creating this tool is to provide investors with real-time insights into the sentiment and market perception surrounding a specific asset mentioned in the news. By quantifying and analyzing the “positive”, “negative”, or “neutral” sentiment expressed in the news, investors can gain a better understanding of how the market is responding to various events and news releases related to the asset. &lt;/p&gt;
&lt;p&gt;This tool would enable investors to quickly identify trends, sentiment shifts, and potential market-moving events, empowering them to make more informed trading decisions. Additionally, it can help investors assess the impact of news sentiment on asset prices and uncover potential trading opportunities or risks. By automating the sentiment analysis process, this tool would also save investors valuable time and effort, freeing them to focus on other aspects of their investment strategies.  &lt;/p&gt;
&lt;p&gt;In a nutshell, the creation of a news sentiment analysis tool specifically designed to follow the news of a particular asset holds immense value for investors. It offers the potential to enhance decision-making, improve timing, and provide a deeper understanding of market sentiment, ultimately aiding investors in achieving their financial objectives. &lt;/p&gt;
&lt;h2&gt;Kickstart with code&lt;/h2&gt;
&lt;p&gt;Why not start with a little experiment, extracting a piece of news to understand the sentiment? Hence, the first step taken was web scrapping to collect textual data. Python modules like selenium to interact with chrome played an important role to locate our target article. Below is a sample of the applied program code: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Setup for selenium &lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.common.by&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;By&lt;/span&gt; 

&lt;span class="c1"&gt;# Prep work for opening Chrome browser &lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ChromeOptions&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 
&lt;span class="n"&gt;hrome_options&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--disable-notifications&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Chrome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# head to webpage of sample article &lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.cnbc.com/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html?&amp;amp;qsearchterm=bitcoin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="c1"&gt;# extract all textual content of the article to a variable &lt;/span&gt;
&lt;span class="n"&gt;article_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_elements&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;By&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;XPATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;//div[@class=&amp;#39;ArticleBody-articleBody&amp;#39;]/div[@class=&amp;#39;group&amp;#39;]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plain_txt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;article_text&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, by utilising the nltk package, an overall sentiment score was calculated, with the value of: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.sentiment.vader&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;SIA&lt;/span&gt; &lt;span class="c1"&gt;# put &amp;quot;positive&amp;quot;, &amp;quot;neutral&amp;quot;, &amp;quot;negative&amp;quot; label to pieces of text &lt;/span&gt;
&lt;span class="n"&gt;sia&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SIA&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# instantiate imported module &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plain_txt&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output: {'neg': 0.0, 'neu': 0.876, 'pos': 0.124, 'compound': 0.9799} &lt;/p&gt;
&lt;p&gt;The high neutral was out of our expectation as we were hoping to be able to identify the potential of a certain asset class after the machine read the news. Simply understanding the tone of news article will not be enough to generalise reliable advice. Hence, further actions should be taken while analysing the news, given that they tend to be objective and remain neutral. &lt;/p&gt;
&lt;h2&gt;Next Step&lt;/h2&gt;
&lt;p&gt;It is suspected that the machine didn’t understand financial content, as it only calculated the sentiment score by general tone of text. Referencing to a similar project conducted by the IMF (Puy, 2019), labels of “bullish” and “bearish” were used while financial-related positive and negative terms were sorted out. Therefore, more research will be conducted to formulate better code for sentiment analysis.  &lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pound, J. (2024, February 28). Bitcoin ETFs see record-high trading volumes as retail investors jump on Crypto Rally. CNBC. &lt;a href="https://www.cnbc.com/amp/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html"&gt;https://www.cnbc.com/amp/2024/02/28/bitcoin-etfs-see-record-high-trading-volumes-as-retail-investors-jump-on-crypto-rally.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Puy, D. (2019, December 16). The power of text: How news sentiment influences financial markets. IMF. &lt;a href="https://www.imf.org/en/Blogs/Articles/2019/12/16/blog-the-power-of-text"&gt;https://www.imf.org/en/Blogs/Articles/2019/12/16/blog-the-power-of-text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog Post"></category><category term="Group NewPotential"></category></entry><entry><title>Intro - ESG News and Stock Volatility (by Group "ESG &amp; Volatility")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/intro-esg-news-and-stock-volatility-by-group-esg-volatility.html" rel="alternate"></link><published>2024-03-04T00:00:00+08:00</published><updated>2024-03-04T00:00:00+08:00</updated><author><name>Group ESG &amp; Volatility</name></author><id>tag:buehlmaier.github.io,2024-03-04:/FINA4350-student-blog-2024-01/intro-esg-news-and-stock-volatility-by-group-esg-volatility.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In our first blog post, we will provide an introduction to our project and offer some insights into our preliminary data collection process.&lt;/p&gt;
&lt;p&gt;The aim of our project is to discover the relationship between ESG (Environmental, social, and governance) news coverage and the volatility of companies. We aim to …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In our first blog post, we will provide an introduction to our project and offer some insights into our preliminary data collection process.&lt;/p&gt;
&lt;p&gt;The aim of our project is to discover the relationship between ESG (Environmental, social, and governance) news coverage and the volatility of companies. We aim to look at different sources, such as major news providers or social media like Twitter, focusing on ESG-related keywords to conduct sentiment analysis to see whether the frequency of mentions affects the volatility of companies. For example, companies that have a higher frequency of negative ESG-related news may portray higher price volatility in the market. Ultimately, we aim to see the level of significance of ESG news in stock prices.&lt;/p&gt;
&lt;p&gt;We came up with this idea because of the growing acknowledgment of ESG factors in business and investment decisions today, and the impact of news on shaping investors’ behaviour in the market. Recently, there has been a surge of news on governance issues of Kakao Corp., one of the prominent growth stocks in Korea. Allegations of stock price manipulation and embezzlement have surfaced, leading to an immediate decline in Kakao's stock price. This example has sparked our motivation to delve deeper into the connection between ESG sentiment scores and their influence on investors and eventually, stock prices.&lt;/p&gt;
&lt;h2&gt;Literature Review&lt;/h2&gt;
&lt;p&gt;Here are some articles we referenced.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0313592623001601"&gt;The Effect of ESG performance on the stock market during the COVID-19 Pandemic — Evidence from Japan&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The methodology section of the article details the use of a firm fixed effects panel model to examine the impact of ESG (environmental, social, and governance) on the returns, volatility, and liquidity of the stocks of more than 300 listed companies in Japan. The study utilizes ESG scores from FTSE Russell and Standard &amp;amp; Poor's to fully assess ESG performance. The article provided us with a methodology on how to establish the link between ESG performance and company stock performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://towardsdatascience.com/spot-vs-sentiment-nlp-sentiment-scoring-in-the-spot-copper-market-492456b031b0"&gt;Trading Sentiment: NLP &amp;amp; Sentiment Scoring the Spot Copper Market&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main objective of the article is to analyze the sentiment behind each tweet and its correlation with the spot price of copper over the past five years from historical tweets posted by financial news publishers on Twitter by using natural language processing. Techniques include data acquisition using GetOldTweets3, Exploratory Data Analysis (EDA), text preprocessing using Stopwords, tokenization, n-grams, Stemming &amp;amp; lemmatization, using GenSim and NLTK PyLDAvis, through Latent Dirichlet Allocation (LDA) model to analyze the data, and sentiment scoring using VADER. The article provides us with information regarding how to extract the data and how to preprocess the text. It also reminds us about the importance of data validation, as data may be lost or valuable data may not be included in the preprocessing process due to improper use of methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://doi.org/10.1007/s12351-023-00745-1"&gt;ESG performance, herding behavior and stock market returns: evidence from Europe&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This research explores the dynamics between ESG performance, investor herding, and stock market outcomes in Europe, analyzing data from large-cap companies across six nations from 2010 to 2020. It assesses how ESG scores and financial metrics like market size, price-to-book value, and the Sharpe ratio influence stock behaviour. The study leverages panel data methodology and sources its ESG information from the Refinitiv Eikon database, offering insights into the financial effects of ESG practices and collective investment patterns in the European markets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://doi.org/10.3390/su14148745"&gt;Proposing an Integrated Approach to Analyzing ESG Data via Machine Learning and Deep Learning Algorithms&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The study discusses a framework that employs artificial intelligence for the analysis of ESG data, underscoring its pivotal role in making well-informed investment decisions, particularly highlighted by the COVID-19 crisis. It explores the application of machine learning and deep learning algorithms to enhance the precision in forecasting companies' ESG performance indicators, such as focusing on governance and social datasets through NLP algorithms. Through various experiments and methods, the research focuses on improving the predictability of ESG scores and the importance of safeguarding ESG data against potential security threats. &lt;/p&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;To perform an analysis of the relationship between ESG news and stock volatility, raw news data is required. We demonstrated a test on news data collection via web scraping.&lt;/p&gt;
&lt;p&gt;&lt;img alt="FinViz News Table" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_01_image-finviz-snap.png"&gt;&lt;/p&gt;
&lt;p&gt;We tried to scrape the news heading from FinViz (a stock screening website). First, we download the HTML for the specific stock and get the news table via requests and BeautifulSoup.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://finviz.com/quote.ashx?t=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;

  &lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;my-app&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lxml&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;news_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;news-table&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;news_table&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, we parse it into a data frame with three columns &lt;code&gt;[‘heading’, ‘datetime’, ‘source’]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[A-Z][a-z]+-\d+-\d+&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_time&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\d+:\d+[A-Z]M&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\(.*\)$&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;map_heading&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;heading&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;source&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ffill&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DatetimeIndex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;source&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heading&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;heading&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_heading&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The format of the output DataFrame is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Output DataFrame" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-esg-and-volatility_01_image-df-snap.png"&gt;&lt;/p&gt;
&lt;p&gt;With a for loop, we can scrap the news headings stock-by-stock and save them into a dict indicated with the stock symbol.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;symbols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MSFT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AMZN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GOOGL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;META&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;NFLX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TSLA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;news&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;symbol&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;symbols&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parse_news_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;news&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This method allows us to collect data from nearly all well-structured HTML such as SeekingAlpha, Yahoo!News, etc. Yet, this will yield too much unnecessary information for the purpose of collecting ESG news, since it scrapes all available news presented on the website without any filtering. One possible solution may be clustering the news data by topic and letting the machine learning algorithms decide what ESG news is, but the work will require a fair amount of time and resources. Therefore, it may be better to filter the ESG news before collecting the data. For example, we may use the search engine or existing libraries to filter the news. In turn, this means we need to use some predefined ESG keywords to perform the search.&lt;/p&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;In our project, we faced significant data collection and analysis challenges. One of the main issues is the sheer volume of data available on news sites and social media platforms related to ESG topics mentioned about companies. The sheer volume of information poses potential problems for data collection, storage, processing, and analysis, so we must improve our data collection methods and focus on the most relevant information. Additionally, changes in social media platforms (e.g., Twitter changed to the X) created problems with data access and the potential for outdated or incomplete datasets. These updates require us to make timely adjustments to our data collection strategy to ensure continued access to up-to-date, comprehensive data. In reflecting on these challenges, we recognize the need to be flexible and innovative in our approach to data collection and analysis, which has allowed us to effectively overcome these barriers.&lt;/p&gt;
&lt;p&gt;Currently, we have identified another challenge that we need to address, which is how to determine if a news article or social media text contains ESG-related content. This is important to ensure that the data we collect for the program is useful. In the previous literature review, one of the studies used the FTSE Russell's ESG Scores and data model, which gives us a well-established catalogue of keywords. For example, for the environment component in ESG, we can look for keywords like “Biodiversity,” “Climate Change,” and “Pollution &amp;amp; Resources.” We are working towards writing code for this section step by step.&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group ESG &amp; Volatility"></category></entry><entry><title>Ideation - What, How, Why, and Why Not (By Group "TheWay")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/ideation-what-how-why-and-why-not-by-group-theway.html" rel="alternate"></link><published>2024-03-03T14:00:00+08:00</published><updated>2024-03-03T14:00:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2024-03-03:/FINA4350-student-blog-2024-01/ideation-what-how-why-and-why-not-by-group-theway.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The very first step of this group project, as always, is to formulate ideas and check their feasibility. In this blog post, we would like to discuss about the few ideas that we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Author: Chau Cheuk Him, Hung Man Kay, Sean Michael Suntoso, Tai Ho Chiu Hero, Wong Ngo Yin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The very first step of this group project, as always, is to formulate ideas and check their feasibility. In this blog post, we would like to discuss about the few ideas that we have. More specifically, we discussed about what exactly the topic is about, how NLP could be involved, and the reasons why the proposed topic should and should not be chosen.&lt;/p&gt;
&lt;h2&gt;Idea 1: ESG Ratings Prediction&lt;/h2&gt;
&lt;h4&gt;ESG Rating and its Significance&lt;/h4&gt;
&lt;p&gt;The Environmental, Social, and Governance (ESG) score is often used to evaluate a company's strength of commitment to sustainability and responsible business practices. It helps socially responsible investors on making investment decisions by providing insights into the company's sustainability performance.&lt;/p&gt;
&lt;p&gt;While ESG rating agencies like MSCI, Sustainalytics and Bloomberg have different methodology, the following are generally considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environmental impact (greenhouse gass emissions, energy efficiency, etc.),&lt;/li&gt;
&lt;li&gt;Social responsibility (diversity and inclusion, product safety, etc.),&lt;/li&gt;
&lt;li&gt;Coperate governance practices (board composition and diversity, shareholder rights, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/whats-esg-score-how-calculated-koviid-sharma/"&gt;What is #ESG Score and How it is Calculated | LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://shorturl.at/tPQ37"&gt;How to Tell If a Company Has High ESG Scores&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Literature Review&lt;/h4&gt;
&lt;p&gt;A growing body of scholarly literature explores the application of natural language processing (NLP) techniques to extract structured data from ESG reports and subsequently analyze them using machine learning models. One such example is the &lt;a href="https://arxiv.org/html/2312.17264v1"&gt;ESGReveal&lt;/a&gt; methodology, which employs an LLM-based approach to harness NLP for the extraction of ESG data from corporate sustainability reports, ultimately generating ESG scores for companies.&lt;/p&gt;
&lt;p&gt;Furthermore, several pre-trained language models that have been finetuned to ESG-related tasks, such as &lt;a href="https://huggingface.co/nbroad/ESG-BERT"&gt;ESGBERT&lt;/a&gt;, are available. ESGBERT can be directly used for text classification/ topic identification in the ESG domain as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;

&lt;span class="c1"&gt;# load tokenizer and model&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbroad/ESG-BERT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbroad/ESG-BERT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# pipeline for text classification&lt;/span&gt;
&lt;span class="n"&gt;text_classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text-classification&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# perform text classification on a sample list of texts&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_classifier&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Our production line releases a lot of carbon dioxide.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Men are paid 3 times more than women.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From the above sample texts, the ESGBERT model detects the presence of the topic "GHG_Emissions" with a probability of 78.11% and "Labor_Practices" with a probability of 95.79%.&lt;/p&gt;
&lt;p&gt;To build a model that, instead of identifying topics from texts, determines the ESG score, one possible solution is to use the ESGBERT model as a base model and further employ transfer learning to finetune it with extra ESG rating data, such that it can capitalize on the knowledge acquired from extensive text data corpora to perform more specific tasks, i.e. to predict ESG ratings for companies.&lt;/p&gt;
&lt;p&gt;Another notable example in the literature is &lt;a href="https://huggingface.co/ai-lab/ESGify"&gt;ESGify&lt;/a&gt;, a machine learning model capable of predicting ESG scores for companies based on their financial data. This model employs a combination of financial ratios and machine learning algorithms to anticipate a company's ESG score.&lt;/p&gt;
&lt;h4&gt;Challenges and Research Value&lt;/h4&gt;
&lt;p&gt;If we go with this topic, there might be a lot of challenges ahead since ESG score is usually done and audited manually. We also keep in mind that ESG is a hot topic that will interfered with investor decision, thus this project can be useful when doing due diligence of a company to invest.&lt;/p&gt;
&lt;h2&gt;Idea 2: Congress Trading Analysis&lt;/h2&gt;
&lt;h4&gt;Background&lt;/h4&gt;
&lt;p&gt;Congress members have gained a huge financial success in the stock market. Many congress members have been out-performing the market for years, consistently. For example, Nancy Pelosi, is known as one of the best-performing fund managers, has a 65% return in the year 2023, beating the benchmark S&amp;amp;P (with a 24% upside) by 2.7 times. Below is a chart showing the return of Congress members VS SPY in 2023. You can see that Congress outperformed SPY a lot in 2023.&lt;/p&gt;
&lt;p&gt;&lt;img src='images/The-Way_01_congressVSspy.png' alt='Congress VS SPY' width='70%'&gt;&lt;/p&gt;
&lt;p&gt;(Image from &lt;a href="https://unusualwhales.com/politics/article/congress-trading-report-2023#tldr"&gt;Congress Trading Report 2023&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;References of similar works&lt;/h4&gt;
&lt;p&gt;By looking into social media and tweets from policy-makers, we may be able to gain some valuable investment insights or even make profits from their trading history. Some previous works could be referenced on this project topic:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/sa1K/Congressional-Stock-Trading/tree/main"&gt;Congress copy-trade Github repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.govinfo.gov/app/collection/crec/2024/01/01-02/3"&gt;Congress Debate History, can be used for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.govtrack.us/"&gt;Another website tracking Congress movement, could be used for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/unitedstates/congress"&gt;This is a community-run project to develop Python tools to collect data about the bills, amendments, roll call votes, and other core data about the U.S. Congress into simple-to-use structured data files.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/SpeakerPelosi"&gt;Twitter of Nancy Pelosi, contains tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sa1K/Congressional-Stock-Trading/tree/main"&gt;Uses Selenium to scrape and make transcations in robinhood based on weighted sum of all politician trades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datacoalition.org/"&gt;Contains Open-Data for US Government&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.capitoltrades.com/trades?per_page=96&amp;amp;politician=P000197#"&gt;Trace Congress party on their equity traded&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Code Skeleton&lt;/h4&gt;
&lt;p&gt;Below are the proposed procedures to process the data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Available datasets can be used directly or after being converted to texts using Optical Character Recognition (OCR). If data are insufficient, web-scraping with Selenium can be done as follows.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.chrome.options&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Options&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scrape&lt;/span&gt;

&lt;span class="c1"&gt;# setting up drivers&lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Options&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_experimental_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;detach&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Chrome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;options&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;chrome_options&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;trades&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# time.sleep(3.5)&lt;/span&gt;
    &lt;span class="n"&gt;trades2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scrape&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trade_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;https://www.capitoltrades.com/trades?per_page=96&amp;amp;page=&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;trades&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;trades&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trades2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ignore_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Write the scrapped content to .csv for further processing&lt;/li&gt;
&lt;li&gt;Data cleaning, dataset merging, data pre-processing&lt;/li&gt;
&lt;li&gt;Perform Natural Language Processing (NLP) model training and testing on the crawled textual data&lt;/li&gt;
&lt;li&gt;Signal generation: A classification problem -&amp;gt; [-1, 1], map it to specific market, perform Sentiment Analysis and provide confidence score [-1,1] to determine to SHORT/LONG the corresponding ETF&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Potential work): Correlation matrix to validate whether there's a correlation between Congress transaction history &amp;amp; the time they deliver the speeches&lt;/p&gt;
&lt;p&gt;OR passing certain acts &amp;amp; equities' price upstrike/downstrike, so we can determine who is the real "smart-money"&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Challenges&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Dataset pre-processing might not be easy as data sources are not same&lt;/li&gt;
&lt;li&gt;Twitter API is not completely free. Free users have limited access to the API functionalities.&lt;/li&gt;
&lt;li&gt;The usefulness of the signal: using NLP to analyse the speech of Congress might not be that useful, in a way that speeches are often delayed (the stock price usually change right after the speech/act is delivered/established). Nevertheless, the correlation matrix might help us distinguish who is the real smart-money, and then we can simply do copy-trade based on majority vote consensus on top-gainers (that has a high win-rate).&lt;/li&gt;
&lt;/ol&gt;</content><category term="Progress Report"></category><category term="Group TheWay"></category></entry><entry><title>Demo Blog Post</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/demo-blog-post.html" rel="alternate"></link><published>2022-01-31T01:12:00+08:00</published><updated>2022-01-31T01:12:00+08:00</updated><author><name>FINA4350 Students 2024</name></author><id>tag:buehlmaier.github.io,2022-01-31:/FINA4350-student-blog-2024-01/demo-blog-post.html</id><summary type="html">&lt;p&gt;By Group "Super NLP"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "Super NLP"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP and recalculated the return and 30 days volatility.&lt;/p&gt;
&lt;p&gt;The code we use is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;myvar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;DF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;XRP-data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;How to Include a Quote&lt;/h2&gt;
&lt;p&gt;As a famous hedge fund manager once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fed watching is a great tool to make money. I have been making all my
gazillions using this technique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How to Include an Image&lt;/h2&gt;
&lt;p&gt;Fed Chair Powell is working hard:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Powell" src="https://buehlmaier.github.io/FINA4350-student-blog-2024-01/images/group-Fintech-Disruption_Powell.jpeg"&gt;&lt;/p&gt;</content><category term="Progress Report"></category><category term="Group Super NLP"></category></entry></feed>